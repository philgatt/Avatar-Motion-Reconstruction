{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import errno\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from lib.utils.tools import *\n",
    "from lib.model.loss import *\n",
    "from lib.model.loss_mesh import *\n",
    "from lib.utils.utils_mesh import *\n",
    "from lib.utils.utils_smpl import *\n",
    "from lib.utils.utils_data import *\n",
    "from lib.utils.learning import *\n",
    "from lib.data.dataset_mesh import MotionSMPL\n",
    "from lib.model.model_mesh import MeshRegressor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lib.data.augmentation import Augmenter3D\n",
    "from lib.utils.tools import read_pkl\n",
    "from lib.utils.utils_data import flip_data, crop_scale\n",
    "from lib.utils.utils_mesh import flip_thetas\n",
    "from lib.utils.utils_smpl import SMPL\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lib.data.datareader_h36m import DataReaderH36M  \n",
    "from lib.data.datareader_mesh import DataReaderMesh  \n",
    "from lib.data.dataset_action import random_move  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SCRIPT IS USED TO EVALUATE THE PREDICTED SMPL VERTICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", type=str, default=\"configs/pretrain.yaml\", help=\"Path to the config file.\")\n",
    "    parser.add_argument('-c', '--checkpoint', default='checkpoint', type=str, metavar='PATH', help='checkpoint directory')\n",
    "    parser.add_argument('-p', '--pretrained', default='checkpoint', type=str, metavar='PATH', help='pretrained checkpoint directory')\n",
    "    parser.add_argument('-r', '--resume', default='', type=str, metavar='FILENAME', help='checkpoint to resume (file name)')\n",
    "    parser.add_argument('-e', '--evaluate', default='', type=str, metavar='FILENAME', help='checkpoint to evaluate (file name)')\n",
    "    parser.add_argument('-freq', '--print_freq', default=100)\n",
    "    parser.add_argument('-ms', '--selection', default='latest_epoch.bin', type=str, metavar='FILENAME', help='checkpoint to finetune (file name)')\n",
    "    parser.add_argument('-sd', '--seed', default=0, type=int, help='random seed')\n",
    "    opts = parser.parse_args()\n",
    "    return opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'data/mesh/mesh_det_pw3d.pkl', 'rb') as f:\n",
    "    dataset_pw3d = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'courtyard_backpack_000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pw3d[\"train\"]['source'][765] # first backpack frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['joint_2d', 'confidence', 'joint_cam', 'smpl_pose', 'smpl_shape', 'img_hw', 'image_id', 'source'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pw3d[\"test\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35515"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_pw3d['test']['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pw3d['test']['source'].count('downtown_arguing_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'downtown_arguing_000',\n",
       " 'downtown_arguing_001',\n",
       " 'downtown_bar_000',\n",
       " 'downtown_bar_001',\n",
       " 'downtown_bus_000',\n",
       " 'downtown_bus_001',\n",
       " 'downtown_cafe_000',\n",
       " 'downtown_cafe_001',\n",
       " 'downtown_car_000',\n",
       " 'downtown_car_001',\n",
       " 'downtown_crossStreets_000',\n",
       " 'downtown_crossStreets_001',\n",
       " 'downtown_downstairs_000',\n",
       " 'downtown_enterShop_000',\n",
       " 'downtown_rampAndStairs_000',\n",
       " 'downtown_rampAndStairs_001',\n",
       " 'downtown_runForBus_000',\n",
       " 'downtown_runForBus_001',\n",
       " 'downtown_runForBus_010',\n",
       " 'downtown_runForBus_011',\n",
       " 'downtown_sitOnStairs_000',\n",
       " 'downtown_sitOnStairs_001',\n",
       " 'downtown_stairs_000',\n",
       " 'downtown_upstairs_000',\n",
       " 'downtown_walkBridge_010',\n",
       " 'downtown_walkUphill_000',\n",
       " 'downtown_walking_000',\n",
       " 'downtown_walking_001',\n",
       " 'downtown_warmWelcome_000',\n",
       " 'downtown_warmWelcome_001',\n",
       " 'downtown_weeklyMarket_000',\n",
       " 'downtown_windowShopping_000',\n",
       " 'flat_guitar_010',\n",
       " 'flat_packBags_000',\n",
       " 'office_phoneCall_000',\n",
       " 'office_phoneCall_001',\n",
       " 'outdoors_fencing_010'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset_pw3d['test']['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE OUT WHY length of dataset_pw3d['test']['source'] AND ALL FRAMES FOR PREDICTED VERTICES ARE NOT THE SAME\n",
    "# I UNDERSTAND IT NOW compose[valid] in /home/philipp/3DPW/readme/predicting.ipynb ias different for every person, so change [iModel] according to which person were looking at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_thetas_dt_arguing = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_cafe_00\\MotionBert\\person_1\\vertices_all_frames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(934, 6890, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_thetas_dt_arguing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 898\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 895\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_bar_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1252\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_bar_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 1206\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_bus_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1066\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_bus_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 1482\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_cafe_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 934\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_cafe_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 1142\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_car_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 811\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_car_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 735\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_crossStreets_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 534\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_crossStreets_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 508\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_downstairs_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 638\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_enterShop_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1385\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_rampAndStairs_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 980\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_rampAndStairs_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 974\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_runForBus_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 647\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_runForBus_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 656\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_runForBus_01\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 746\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_runForBus_01\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 692\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_sitOnStairs_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1302\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_sitOnStairs_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 1329\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_stairs_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1197\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_upstairs_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 825\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_walkBridge_01\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1182\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_walking_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1330\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_walking_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 1264\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_walkUphill_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 387\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_warmWelcome_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 568\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_warmWelcome_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 554\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_weeklyMarket_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1055\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_windowShopping_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1824\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\flat_guitar_01\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 748\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\flat_packBags_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 1273\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\office_phoneCall_00\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 789\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\office_phoneCall_00\\MotionBert\\person_2\\vertices_all_frames.npy | Frames: 768\n",
      "Found file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\outdoors_fencing_01\\MotionBert\\person_1\\vertices_all_frames.npy | Frames: 939\n",
      "\n",
      "Total number of frames: 35515\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def count_total_frames(base_path):\n",
    "    \"\"\"\n",
    "    Recursively counts the total number of frames from vertices_all_frames.npy\n",
    "    files in person_1 and person_2 folders.\n",
    "\n",
    "    Parameters:\n",
    "        base_path (str): The root folder path where the search begins.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of frames summed across all files.\n",
    "    \"\"\"\n",
    "    total_frames = 0  # Initialize total frame counter\n",
    "\n",
    "    # Walk through all subdirectories in the base_path\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Check if we're in a person_1 or person_2 folder\n",
    "        if os.path.basename(root) in ['person_1', 'person_2']:\n",
    "            # Check if 'vertices_all_frames.npy' exists in the current folder\n",
    "            file_path = os.path.join(root, 'vertices_all_frames.npy')\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    # Load the numpy array and get the number of frames (first dimension)\n",
    "                    vertices_data = np.load(file_path)\n",
    "                    nr_frames = vertices_data.shape[0]  # First dimension is number of frames\n",
    "                    total_frames += nr_frames\n",
    "                    print(f\"Found file: {file_path} | Frames: {nr_frames}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal number of frames: {total_frames}\")\n",
    "    return total_frames\n",
    "\n",
    "\n",
    "\n",
    "# Define the base folder path to search in\n",
    "base_folder = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "\n",
    "# Run the frame counting function\n",
    "total_frames = count_total_frames(base_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'configs/mesh/MB_train_pw3d.yaml'\n",
    "args = get_config(config)\n",
    "\n",
    "\n",
    "trainloader_params = {\n",
    "        'batch_size': args.batch_size,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0,\n",
    "        'pin_memory': True,\n",
    "}\n",
    "testloader_params = {\n",
    "        'batch_size': args.batch_size,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0,\n",
    "        'pin_memory': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SMPLDataset(Dataset):\n",
    "    def __init__(self, args, data_split, dataset): # data_split: train/test; dataset: h36m, coco, pw3d\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.clip_len = args.clip_len\n",
    "        self.data_split = data_split\n",
    "    \n",
    "        if dataset==\"h36m\":\n",
    "            datareader = DataReaderH36M(n_frames=self.clip_len, sample_stride=args.sample_stride, data_stride_train=args.data_stride, data_stride_test=self.clip_len, dt_root=args.data_root, dt_file=args.dt_file_h36m)\n",
    "        elif dataset==\"coco\":\n",
    "            datareader = DataReaderMesh(n_frames=1, sample_stride=args.sample_stride, data_stride_train=1, data_stride_test=1, dt_root=args.data_root, dt_file=args.dt_file_coco, res=[640, 640])\n",
    "        elif dataset==\"pw3d\":\n",
    "            datareader = DataReaderMesh(n_frames=self.clip_len, sample_stride=args.sample_stride, data_stride_train=args.data_stride, data_stride_test=self.clip_len, dt_root=args.data_root, dt_file=args.dt_file_pw3d, res=[1920, 1920])\n",
    "        else:\n",
    "            raise Exception(\"Mesh dataset undefined.\")\n",
    "        self.datareader = datareader\n",
    "\n",
    "        split_id_train, split_id_test = datareader.get_split_id()                        # Index of clips\n",
    "        # print(\"ID TRAIN:\")\n",
    "        # print(split_id_train)\n",
    "        # print(\"ID TEST:\")\n",
    "        # print(split_id_test)\n",
    "        train_data, test_data = datareader.read_2d()\n",
    "        train_data, test_data = train_data[split_id_train], test_data[split_id_test]     # Input: (N, T, 17, 3)\n",
    "        self.motion_2d = {'train': train_data, 'test': test_data}[data_split]\n",
    "\n",
    "        dt = datareader.dt_dataset\n",
    "        smpl_pose_train = dt['train']['smpl_pose'][split_id_train]                       # (N, T, 72)\n",
    "        smpl_shape_train = dt['train']['smpl_shape'][split_id_train]                     # (N, T, 10)\n",
    "        smpl_pose_test = dt['test']['smpl_pose'][split_id_test]                          # (N, T, 72)\n",
    "        smpl_shape_test = dt['test']['smpl_shape'][split_id_test]                        # (N, T, 10)\n",
    "        \n",
    "        self.motion_smpl_3d = {'train': {'pose': smpl_pose_train, 'shape': smpl_shape_train}, 'test': {'pose': smpl_pose_test, 'shape': smpl_shape_test}}[data_split]\n",
    "        self.smpl = SMPL(\n",
    "            args.data_root,\n",
    "            batch_size=1,\n",
    "        )\n",
    "    def get_split_id(self, datareader):\n",
    "        return datareader.get_split_id()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.motion_2d)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError \n",
    "\n",
    "class MotionSMPL(SMPLDataset):\n",
    "    def __init__(self, args, data_split, dataset):\n",
    "        super(MotionSMPL, self).__init__(args, data_split, dataset)\n",
    "        self.flip = args.flip\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        motion_2d = self.motion_2d[index]                                            # motion_2d: (T,17,3)     \n",
    "        motion_2d[:,:,2] = np.clip(motion_2d[:,:,2], 0, 1)\n",
    "        motion_smpl_pose = self.motion_smpl_3d['pose'][index].reshape(-1, 24, 3)     # motion_smpl_3d: (T, 24, 3)    \n",
    "        motion_smpl_shape = self.motion_smpl_3d['shape'][index]                      # motion_smpl_3d: (T,10)    \n",
    "        \n",
    "        # if self.data_split==\"train\":\n",
    "        #     if self.flip and random.random() > 0.5:                                  # Training augmentation - random flipping\n",
    "        #         motion_2d = flip_data(motion_2d)\n",
    "        #         motion_smpl_pose = flip_thetas(motion_smpl_pose)                \n",
    "\n",
    "            \n",
    "        motion_smpl_pose = torch.from_numpy(motion_smpl_pose).reshape(-1, 72).float()\n",
    "        motion_smpl_shape = torch.from_numpy(motion_smpl_shape).reshape(-1, 10).float()\n",
    "        motion_smpl = self.smpl(\n",
    "            betas=motion_smpl_shape,\n",
    "            body_pose=motion_smpl_pose[:, 3:],\n",
    "            global_orient=motion_smpl_pose[:, :3],\n",
    "            pose2rot=True\n",
    "        )\n",
    "        motion_verts = motion_smpl.vertices.detach()*1000.0\n",
    "        J_regressor = self.smpl.J_regressor_h36m\n",
    "        J_regressor_batch = J_regressor[None, :].expand(motion_verts.shape[0], -1, -1).to(motion_verts.device)\n",
    "        motion_3d_reg = torch.matmul(J_regressor_batch, motion_verts)                 # motion_3d: (T,17,3)  \n",
    "        motion_verts = motion_verts - motion_3d_reg[:, :1, :]\n",
    "        motion_3d_reg = motion_3d_reg - motion_3d_reg[:, :1, :]                       # motion_3d: (T,17,3)    \n",
    "        motion_theta = torch.cat((motion_smpl_pose, motion_smpl_shape), -1)\n",
    "        motion_smpl_3d = {\n",
    "            'theta': motion_theta,       # smpl pose and shape\n",
    "            'kp_3d': motion_3d_reg,      # 3D keypoints\n",
    "            'verts': motion_verts,       # 3D mesh vertices\n",
    "        }\n",
    "        return motion_2d, motion_smpl_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "mesh_train_pw3d = MotionSMPL(args, data_split='train', dataset=\"pw3d\")\n",
    "train_loader_pw3d = DataLoader(mesh_train_pw3d, **trainloader_params)\n",
    "\n",
    "mesh_val_pw3d = MotionSMPL(args, data_split='test', dataset=\"pw3d\")\n",
    "test_loader_pw3d = DataLoader(mesh_val_pw3d, **testloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mesh_train_pw3d[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2202"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mesh_val_pw3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    mesh_train_pw3d = MotionSMPL(args, data_split='train', dataset=\"pw3d\")\n",
    "    train_loader_pw3d = DataLoader(mesh_train_pw3d, **trainloader_params)\n",
    "\n",
    "    mesh_val_pw3d = MotionSMPL(args, data_split='test', dataset=\"pw3d\")\n",
    "    test_loader_pw3d = DataLoader(mesh_val_pw3d, **testloader_params)\n",
    "\n",
    "# Call the main function\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_id_train, split_id_test = mesh_train_pw3d.get_split_id(mesh_train_pw3d.datareader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves the ground truth coords for any image \n",
    "\n",
    "def get_frame_coords(dataset:str, file_name: str, frame_number: int, split_id_train, mesh_train_pw3d):\n",
    "    index = None\n",
    "    resulting_coords = None  # Initialize resulting_coords to avoid potential errors\n",
    "    \n",
    "    # Find the first appearance of the file_name\n",
    "    if dataset == 'train':\n",
    "        img_names = dataset_pw3d[\"train\"][\"source\"]\n",
    "    else:\n",
    "        img_names = dataset_pw3d[\"test\"][\"source\"]\n",
    "\n",
    "    for idx, file in enumerate(img_names):\n",
    "        if file == file_name:\n",
    "            index = idx\n",
    "            break  # Exit the loop as soon as the file is found\n",
    "    else:\n",
    "        # This executes if no break occurs in the loop (file not found)\n",
    "        print(\"File not found\")\n",
    "        return None, None\n",
    "\n",
    "    # Calculate the target frame number\n",
    "    target_number = index + frame_number\n",
    "    \n",
    "    # Find the range index\n",
    "    range_index = next((i for i, r in enumerate(split_id_train) if r.start <= target_number < r.stop), None)\n",
    "    \n",
    "    if range_index is not None:\n",
    "        print(f\"The number {target_number} is in range at index {range_index}: {split_id_train[range_index]}\")\n",
    "        # Calculate the index inside the range\n",
    "        range_start = split_id_train[range_index].start\n",
    "        index_in_range = target_number - range_start\n",
    "        \n",
    "        # Access the batch and ensure indices are valid\n",
    "        if range_index < len(mesh_train_pw3d) and index_in_range < len(mesh_train_pw3d[range_index][1]['kp_3d']):\n",
    "            resulting_coords = mesh_train_pw3d[range_index][1]['kp_3d'][index_in_range]\n",
    "        else:\n",
    "            print(f\"Index {index_in_range} is out of bounds for range index {range_index}.\")\n",
    "    else:\n",
    "        print(f\"The number {target_number} is not in any range of split_id_train.\")\n",
    "        return None, None\n",
    "\n",
    "    return resulting_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found\n"
     ]
    }
   ],
   "source": [
    "resulting_coords = get_frame_coords(\"test\", 'office_phoneCall_00', 98, split_id_test, mesh_val_pw3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mesh_train_pw3d[batch nr][# keypoints (0), smpl stuff (1)]['kp_3d'][#nr_of frame in the batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # Example tensor of shape 17x3 (replace this with your tensor)\n",
    "# coordinates = resulting_coords # Replace with your actual tensor\n",
    "\n",
    "# # Convert tensor to numpy for Matplotlib compatibility\n",
    "# coordinates_np = coordinates.numpy()\n",
    "\n",
    "# # Extract X, Y, Z components\n",
    "# x = coordinates_np[:, 0]\n",
    "# y = coordinates_np[:, 1]\n",
    "# z = coordinates_np[:, 2]\n",
    "\n",
    "# # Create a 3D plot\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Scatter plot of the 3D points\n",
    "# ax.scatter(x, y, z, c='b', marker='o', label=\"3D Points\")\n",
    "\n",
    "# # Add labels\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "# ax.set_title('3D Coordinates Plot')\n",
    "\n",
    "# # Add a legend\n",
    "# ax.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"regressed_coordinates/coordinates_dataset_motionbert.npy\", coordinates_np.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make predictions \n",
    "# #...\n",
    "\n",
    "# # Mesh regress vertices \n",
    "# dataset_pw3d[\"test\"].keys()\n",
    "# mesh_train_pw3d = MotionSMPL(args, data_split='train', dataset=\"pw3d\")\n",
    "# mesh_test_pw3d = MotionSMPL(args, data_split='test', dataset=\"pw3d\")\n",
    "\n",
    "# train_loader_pw3d = DataLoader(mesh_test_pw3d, **trainloader_params)\n",
    "# test_loader_pw3d = DataLoader(mesh_train_pw3d, **testloader_params)\n",
    "\n",
    "# J_regressor =np.load(\"Joint_regressor/J_regressor_h36m_correct.npy\")\n",
    "\n",
    "\n",
    "# # Actually, here, we need to regress the keypoints instead of just using the GT\n",
    "\n",
    "# predicted_coordinates = mesh_test_pw3d[1][1]\n",
    "\n",
    "# output = {\n",
    "#     'theta': None,\n",
    "#     'verts': None,\n",
    "#     'kp_3d': None\n",
    "\n",
    "# }\n",
    "\n",
    "\n",
    "# # Adding a new dimension to each array\n",
    "# output['theta'] = np.expand_dims(mesh_test_pw3d[0][1][\"theta\"].numpy(), axis=0)  # Shape: (1, 16, 72)\n",
    "# output['verts'] = np.expand_dims(mesh_test_pw3d[0][1][\"verts\"].numpy(), axis=0)  # Shape: (1, 16, 6890, 3)\n",
    "# output['kp_3d'] = np.expand_dims(mesh_test_pw3d[0][1][\"kp_3d\"].numpy(), axis=0)  # Shape: (1, 16, 17, 3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 18\n"
     ]
    }
   ],
   "source": [
    "batch_count = 0\n",
    "\n",
    "for batch in test_loader_pw3d:\n",
    "    batch_count += 1\n",
    "    \n",
    "print(f\"Total number of batches: {batch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:25,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, (batch_input, batch_gt) in tqdm(enumerate(test_loader_pw3d)):\n",
    "    batch_size, clip_len = batch_input.shape[:2]\n",
    "    if torch.cuda.is_available():\n",
    "        batch_gt['theta'] = batch_gt['theta'].cuda().float()\n",
    "        batch_gt['kp_3d'] = batch_gt['kp_3d'].cuda().float()\n",
    "        batch_gt['verts'] = batch_gt['verts'].cuda().float()\n",
    "        batch_input = batch_input.cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 16, 82])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gt['theta'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "theta = torch.randn(batch_gt['theta'].shape[0], batch_gt['theta'].shape[1], batch_gt['theta'].shape[2])\n",
    "kp_3d = torch.randn(batch_gt['kp_3d'].shape[0], batch_gt['kp_3d'].shape[0], batch_gt['kp_3d'].shape[0], batch_gt['kp_3d'].shape[0])\n",
    "verts = torch.randn(batch_gt['verts'].shape[0], batch_gt['verts'].shape[0], batch_gt['verts'].shape[0], batch_gt['verts'].shape[0])\n",
    "\n",
    "# Create the dictionary\n",
    "sample_prediction = {\n",
    "    \"theta\": theta,\n",
    "    \"kp_3d\": kp_3d,\n",
    "    \"verts\": verts,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 16, 82]) torch.Size([26, 26, 26, 26]) torch.Size([26, 26, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "print(theta.shape, kp_3d.shape, verts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"output is a dict of shape\"\n",
    "# theta: (1, T, dim_theta) (e.g., (1, T, 72))\n",
    "# verts: (1, T, num_verts, 3) (e.g., (1, T, 6890, 3) if num_verts=6890)\n",
    "# kp_3d: (1, T, 17, 3)\n",
    "\n",
    "\n",
    "\n",
    "def validate(test_loader, output, criterion, dataset_name='h36m'):\n",
    "\n",
    "    print(f'===========> validating {dataset_name}')\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_dict = {'loss_3d_pos': AverageMeter(), \n",
    "                   'loss_3d_scale': AverageMeter(), \n",
    "                   'loss_3d_velocity': AverageMeter(),\n",
    "                   'loss_lv': AverageMeter(), \n",
    "                   'loss_lg': AverageMeter(), \n",
    "                   'loss_a': AverageMeter(), \n",
    "                   'loss_av': AverageMeter(), \n",
    "                   'loss_pose': AverageMeter(), \n",
    "                   'loss_shape': AverageMeter(),\n",
    "                   'loss_norm': AverageMeter(),\n",
    "    }\n",
    "    mpjpes = AverageMeter()\n",
    "    mpves = AverageMeter()\n",
    "    results = defaultdict(list)\n",
    "    smpl = SMPL(args.data_root, batch_size=1).cuda()\n",
    "    J_regressor = smpl.J_regressor_h36m\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (batch_input, batch_gt) in tqdm(enumerate(test_loader)):\n",
    "            batch_size, clip_len = batch_input.shape[:2]\n",
    "            if torch.cuda.is_available():\n",
    "                batch_gt['theta'] = batch_gt['theta'].cuda().float()\n",
    "                batch_gt['kp_3d'] = batch_gt['kp_3d'].cuda().float()\n",
    "                batch_gt['verts'] = batch_gt['verts'].cuda().float()\n",
    "                batch_input = batch_input.cuda().float()\n",
    "\n",
    "            theta = torch.randn(batch_gt['theta'].shape[0], batch_gt['theta'].shape[1], batch_gt['theta'].shape[2])\n",
    "            kp_3d = torch.randn(batch_gt['kp_3d'].shape[0], batch_gt['kp_3d'].shape[1], batch_gt['kp_3d'].shape[2], batch_gt['kp_3d'].shape[3])\n",
    "            verts = torch.randn(batch_gt['verts'].shape[0], batch_gt['verts'].shape[1], batch_gt['verts'].shape[2], batch_gt['verts'].shape[3])\n",
    "\n",
    "            theta = theta.to('cuda')\n",
    "            kp_3d = kp_3d.to('cuda')\n",
    "            verts = verts.to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "            # Create the dictionary\n",
    "            sample_prediction = {\n",
    "                \"theta\": theta,\n",
    "                \"kp_3d\": kp_3d,\n",
    "                \"verts\": verts,\n",
    "            }\n",
    "            \n",
    "            output = [sample_prediction]\n",
    "\n",
    "            output_final = output\n",
    "            \n",
    "            print('Pred is')\n",
    "            print(output[0]['kp_3d'].shape)\n",
    "\n",
    "            print('GT is')\n",
    "            print(batch_gt['kp_3d'].shape)\n",
    "\n",
    "            loss_dict = criterion(output, batch_gt)\n",
    "            loss = args.lambda_3d      * loss_dict['loss_3d_pos']      + \\\n",
    "                   args.lambda_scale   * loss_dict['loss_3d_scale']    + \\\n",
    "                   args.lambda_3dv     * loss_dict['loss_3d_velocity'] + \\\n",
    "                   args.lambda_lv      * loss_dict['loss_lv']          + \\\n",
    "                   args.lambda_lg      * loss_dict['loss_lg']          + \\\n",
    "                   args.lambda_a       * loss_dict['loss_a']           + \\\n",
    "                   args.lambda_av      * loss_dict['loss_av']          + \\\n",
    "                   args.lambda_shape   * loss_dict['loss_shape']       + \\\n",
    "                   args.lambda_pose    * loss_dict['loss_pose']        + \\\n",
    "                   args.lambda_norm    * loss_dict['loss_norm'] \n",
    "            # update metric\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            loss_str = ''\n",
    "            for k, v in loss_dict.items():\n",
    "                losses_dict[k].update(v.item(), batch_size)\n",
    "                loss_str += '{0} {loss.val:.3f} ({loss.avg:.3f})\\t'.format(k, loss=losses_dict[k])\n",
    "            mpjpe, mpve = compute_error(output, batch_gt)\n",
    "            mpjpes.update(mpjpe, batch_size)\n",
    "            mpves.update(mpve, batch_size)\n",
    "            \n",
    "            for keys in output[0].keys():\n",
    "                if type(output[0][keys]) != np.ndarray: \n",
    "                    output[0][keys] = output[0][keys].detach().cpu().numpy()\n",
    "                if type(batch_gt[keys]) != np.ndarray: \n",
    "                    batch_gt[keys] = batch_gt[keys].detach().cpu().numpy()\n",
    "            results['kp_3d'].append(output[0]['kp_3d'])\n",
    "            results['verts'].append(output[0]['verts'])\n",
    "            results['kp_3d_gt'].append(batch_gt['kp_3d'])\n",
    "            results['verts_gt'].append(batch_gt['verts'])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if idx % int(100) == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      '{2}'\n",
    "                      'PVE {mpves.val:.3f} ({mpves.avg:.3f})\\t'\n",
    "                      'JPE {mpjpes.val:.3f} ({mpjpes.avg:.3f})'.format(\n",
    "                       idx, len(test_loader), loss_str, batch_time=batch_time,\n",
    "                       loss=losses, mpves=mpves, mpjpes=mpjpes))\n",
    "            \n",
    "\n",
    "    print(f'==> start concating results of {dataset_name}')\n",
    "    for term in results.keys():\n",
    "        results[term] = np.concatenate(results[term])\n",
    "    print(f'==> start evaluating {dataset_name}...')\n",
    "    error_dict = evaluate_mesh(results)\n",
    "    err_str = ''\n",
    "    for err_key, err_val in error_dict.items():\n",
    "        err_str += '{}: {:.2f}mm \\t'.format(err_key, err_val)\n",
    "    print(f'=======================> {dataset_name} validation done: ', loss_str)\n",
    "    print(f'=======================> {dataset_name} validation done: ', err_str)\n",
    "    return losses.avg, error_dict['mpjpe'], error_dict['pa_mpjpe'], error_dict['mpve'], losses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MeshLoss(loss_type = args.loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========> validating pw3d\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/18]\tTime 1.988 (1.988)\tLoss 1009.5448 (1009.5448)\tloss_3d_pos 407.695 (407.695)\tloss_3d_scale 404.192 (404.192)\tloss_3d_velocity 6.694 (6.694)\tloss_lv 0.908 (0.908)\tloss_lg 239.669 (239.669)\tloss_a 1.201 (1.201)\tloss_av 0.685 (0.685)\tloss_shape 0.984 (0.984)\tloss_pose 0.557 (0.557)\tloss_norm 9.016 (9.016)\tPVE 455.068 (455.068)\tJPE 407.695 (407.695)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:03,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:05,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:07,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:09,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:10,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:12,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:14,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:16,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:18,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:19,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:23,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:25,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:27,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:28,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:30,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([128, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([128, 16, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:31,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred is\n",
      "torch.Size([26, 16, 17, 3])\n",
      "GT is\n",
      "torch.Size([26, 16, 17, 3])\n",
      "==> start concating results of pw3d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> start evaluating pw3d...\n",
      "=======================> pw3d validation done:  loss_3d_pos 410.785 (425.226)\tloss_3d_scale 406.593 (419.881)\tloss_3d_velocity 5.649 (10.397)\tloss_lv 0.938 (0.907)\tloss_lg 250.634 (254.918)\tloss_a 1.191 (1.169)\tloss_av 0.693 (0.687)\tloss_shape 1.086 (1.070)\tloss_pose 0.561 (0.561)\tloss_norm 9.113 (9.028)\t\n",
      "=======================> pw3d validation done:  mpve: 472.44mm \tmpjpe: 458.91mm \tpa_mpjpe: 416.31mm \tmpjpe_17j: 425.23mm \tpa_mpjpe_17j: 393.10mm \t\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     mesh_train_pw3d = MotionSMPL(args, data_split='train', dataset=\"pw3d\")\n",
    "#     mesh_test_pw3d = MotionSMPL(args, data_split='test', dataset=\"pw3d\")\n",
    "\n",
    "#     train_loader_pw3d = DataLoader(mesh_train_pw3d, **trainloader_params)\n",
    "#     test_loader_pw3d = DataLoader(mesh_test_pw3d, **testloader_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_mpjpe, test_pa_mpjpe, test_mpve, test_losses_dict = validate(test_loader_pw3d, output=None, criterion=criterion, dataset_name='pw3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "smpl = SMPL(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBert\\MotionBERT\\data\\mesh\", batch_size=1).cuda()\n",
    "J_regressor = smpl.J_regressor_h36m\n",
    "\n",
    "J_regressor_motionbert = np.load(\"Joint_regressor/J_regressor_h36m_correct.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # Ground Truth joint coordinates\n",
    "\n",
    "# # Reshape the list into a 24x3 array\n",
    "# coordinates = np.array(joint_positions_gt).reshape(24, 3)\n",
    "\n",
    "# # Extract x, y, z coordinates\n",
    "# x = coordinates[:, 0]\n",
    "# y = coordinates[:, 1]\n",
    "# z = coordinates[:, 2]\n",
    "\n",
    "# # Plot the 3D points\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(x, y, z, c='r', marker='o', label='Joint Positions')\n",
    "\n",
    "# # Optional: Add labels and a legend\n",
    "# ax.set_xlabel('X Coordinate')\n",
    "# ax.set_ylabel('Y Coordinate')\n",
    "# ax.set_zlabel('Z Coordinate')\n",
    "# ax.set_title('3D Joint Positions')\n",
    "# ax.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Regressed coords for GT SMPL and for predicted SMPL\n",
    "\n",
    "# GT = False\n",
    "\n",
    "# if GT:\n",
    "#     # Extract X, Y, Z coordinates\n",
    "#     x_coords = result_gt[:, 0]\n",
    "#     y_coords = result_gt[:, 1]\n",
    "#     z_coords = result_gt[:, 2]\n",
    "# else:\n",
    "#     # Extract X, Y, Z coordinates\n",
    "#     x_coords = result_pred[:, 0]\n",
    "#     y_coords = result_pred[:, 1]\n",
    "#     z_coords = result_pred[:, 2]\n",
    "\n",
    "# # Plot in 3D\n",
    "# fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(x_coords, y_coords, z_coords, c='r', marker='o', label=\"Joints\")\n",
    "\n",
    "# # Label the axes\n",
    "# ax.set_xlabel('X Coordinate')\n",
    "# ax.set_ylabel('Y Coordinate')\n",
    "# ax.set_zlabel('Z Coordinate')\n",
    "\n",
    "# # Add a title and legend\n",
    "# ax.set_title(\"3D Joint Positions\")\n",
    "# ax.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"Regressed_Coordinates/regressed_backpack_pred_00.npy\",result_pred.flatten())\n",
    "# #Currently finding out what the difference is between the original ground truth keypoints, and the keypoints I get when I regress the ground truth SMPL vertices set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Take vertices ground truth of backpack first frame and regress here. Then plot it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing backpack example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_regressor_motionbert = np.load(\"Joint_regressor/J_regressor_h36m_correct.npy\")\n",
    "\n",
    "\n",
    "def thetas_to_kp_3d(pred_thetas, pred_verts, J_regressor):\n",
    "    pred_thetas = np.concatenate([pred_thetas, np.zeros(10)])\n",
    "    pred_kp_3d = J_regressor @ pred_verts\n",
    "\n",
    "\n",
    "    # Define the rotation matrix for -180 degrees around the x-axis\n",
    "    R_x = np.array([\n",
    "        [1,  0,  0],\n",
    "        [0, -1,  0],\n",
    "        [0,  0, -1]\n",
    "    ])\n",
    "\n",
    "    # Rotate each point by applying the rotation matrix\n",
    "    pred_kp_3d = pred_kp_3d @ R_x.T\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_thetas = np.load('Prediction_necessities/all_thetas_backpack.npy')[0]\n",
    "pred_thetas = np.concatenate([pred_thetas, np.zeros(10)])\n",
    "pred_verts = np.load('Prediction_necessities/vertices_backpack_frame_00_pred.npy')\n",
    "#J_regressor = J_regressor.to('cpu')\n",
    "pred_kp_3d = J_regressor_motionbert @ pred_verts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rotation matrix for -180 degrees around the x-axis\n",
    "R_x = np.array([\n",
    "    [1,  0,  0],\n",
    "    [0, -1,  0],\n",
    "    [0,  0, -1]\n",
    "])\n",
    "\n",
    "# Rotate each point by applying the rotation matrix\n",
    "pred_kp_3d = pred_kp_3d @ R_x.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first coordinate\n",
    "first_point = pred_kp_3d[0]\n",
    "\n",
    "# Translate all points to set the first point as the origin\n",
    "pred_kp_3d = pred_kp_3d - first_point\n",
    "\n",
    "# Ensure the first point is now exactly zero\n",
    "pred_kp_3d[0] = np.array([0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "pred_thetas = torch.tensor(pred_thetas).unsqueeze(0).unsqueeze(0)\n",
    "pred_verts = torch.tensor(pred_verts).unsqueeze(0).unsqueeze(0)\n",
    "pred_kp_3d = torch.tensor(pred_kp_3d).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move tensors to GPU\n",
    "pred_thetas = pred_thetas.to(device)\n",
    "pred_verts = pred_verts.to(device)\n",
    "pred_kp_3d = pred_kp_3d.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = {\n",
    "    \"theta\": pred_thetas,\n",
    "    \"kp_3d\": pred_kp_3d,\n",
    "    \"verts\": pred_verts,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 17, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_kp_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting_coords = get_frame_coords(\"train\", 'courtyard_backpack_000', 0, split_id_train, mesh_train_pw3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Regressed_Coordinates/regressed_backpack_origin.npy', (sample_prediction['kp_3d']*1000).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('Regressed_Coordinates/ground_truth_coords.npy', resulting_coords.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader_pw3d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.0000,    0.0000,    0.0000],\n",
       "        [-140.4294,    1.5662,  -45.2371],\n",
       "        [ -75.8165,  430.4894,   21.4302],\n",
       "        [ -97.8810,  825.3510,  165.0902],\n",
       "        [ 139.2287,   -0.9688,   44.7472],\n",
       "        [ 117.8244,  438.8577,   61.3678],\n",
       "        [  88.7600,  817.7072,  217.9599],\n",
       "        [ -12.2836, -249.0143,   -9.4596],\n",
       "        [   3.7105, -480.3842,  -85.9170],\n",
       "        [  36.8778, -508.3334, -181.2408],\n",
       "        [  43.1729, -616.3802, -216.4468],\n",
       "        [ 133.6753, -435.5518,  -16.6594],\n",
       "        [ 223.1522, -189.5069,   75.7827],\n",
       "        [ 195.9183,   58.6210,   92.5941],\n",
       "        [-136.4164, -425.4830, -106.7033],\n",
       "        [-237.8101, -171.6483,  -67.3985],\n",
       "        [-198.9982,   70.9759,  -42.6763]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These two are the same \n",
    "mesh_train_pw3d[94][1]['kp_3d'][0]\n",
    "batch[1]['kp_3d'][94][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number 765 is in range at index 94: range(765, 781)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   0.0000,    0.0000,    0.0000],\n",
       "        [-140.4294,    1.5662,  -45.2371],\n",
       "        [ -75.8165,  430.4894,   21.4302],\n",
       "        [ -97.8810,  825.3510,  165.0902],\n",
       "        [ 139.2287,   -0.9688,   44.7472],\n",
       "        [ 117.8244,  438.8577,   61.3678],\n",
       "        [  88.7600,  817.7072,  217.9599],\n",
       "        [ -12.2836, -249.0143,   -9.4596],\n",
       "        [   3.7105, -480.3842,  -85.9170],\n",
       "        [  36.8778, -508.3334, -181.2408],\n",
       "        [  43.1729, -616.3802, -216.4468],\n",
       "        [ 133.6753, -435.5518,  -16.6594],\n",
       "        [ 223.1522, -189.5069,   75.7827],\n",
       "        [ 195.9183,   58.6210,   92.5941],\n",
       "        [-136.4164, -425.4830, -106.7033],\n",
       "        [-237.8101, -171.6483,  -67.3985],\n",
       "        [-198.9982,   70.9759,  -42.6763]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_frame_coords(\"train\", 'courtyard_backpack_000', 0, split_id_train, mesh_train_pw3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 82])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_prediction[\"theta\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda-envs\\motionbert\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "e:\\miniconda-envs\\motionbert\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "e:\\miniconda-envs\\motionbert\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "e:\\miniconda-envs\\motionbert\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Extract, reshape, and convert batch[0]\n",
    "batch[0] = batch[0][0][0]\n",
    "batch[0] = torch.tensor(batch[0]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "# Extract, reshape, and convert batch[1] elements\n",
    "batch[1]['theta'] = torch.tensor(batch[1]['theta'][0][0]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "batch[1]['kp_3d'] = torch.tensor(batch[1]['kp_3d'][0][0]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "batch[1]['verts'] = torch.tensor(batch[1]['verts'][0][0]).unsqueeze(0).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 17, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]['kp_3d'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 17, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"output is a dict of shape\"\n",
    "# theta: (1, T, dim_theta) (e.g., (1, T, 72))\n",
    "# verts: (1, T, num_verts, 3) (e.g., (1, T, 6890, 3) if num_verts=6890)\n",
    "# kp_3d: (1, T, 17, 3)\n",
    "\n",
    "\n",
    "\n",
    "def validate(criterion, dataset_name='h36m'):\n",
    "\n",
    "    print(f'===========> validating {dataset_name}')\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_dict = {'loss_3d_pos': AverageMeter(), \n",
    "                   'loss_3d_scale': AverageMeter(), \n",
    "                   'loss_3d_velocity': AverageMeter(),\n",
    "                   'loss_lv': AverageMeter(), \n",
    "                   'loss_lg': AverageMeter(), \n",
    "                   'loss_a': AverageMeter(), \n",
    "                   'loss_av': AverageMeter(), \n",
    "                   'loss_pose': AverageMeter(), \n",
    "                   'loss_shape': AverageMeter(),\n",
    "                   'loss_norm': AverageMeter(),\n",
    "    }\n",
    "    mpjpes = AverageMeter()\n",
    "    mpves = AverageMeter()\n",
    "    results = defaultdict(list)\n",
    "    smpl = SMPL(args.data_root, batch_size=1).cuda()\n",
    "    J_regressor = smpl.J_regressor_h36m\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "    ######################################################################\n",
    "    idx = 0\n",
    "    batch_input = batch[0]\n",
    "    batch_gt = batch[1]\n",
    "\n",
    "    batch_size, clip_len = batch_input.shape[:2]\n",
    "    if torch.cuda.is_available():\n",
    "        batch_gt['theta'] = batch_gt['theta'].cuda().float()\n",
    "        batch_gt['kp_3d'] = batch_gt['kp_3d'].cuda().float()\n",
    "        batch_gt['verts'] = batch_gt['verts'].cuda().float()\n",
    "        batch_input = batch_input.cuda().float()\n",
    "\n",
    "    output = [sample_prediction]\n",
    "\n",
    "    output_final = output\n",
    "\n",
    "    print('Pred is')\n",
    "    print(output[0]['kp_3d'].shape)\n",
    "\n",
    "    print('GT is')\n",
    "    print(batch_gt['kp_3d'].shape)\n",
    "\n",
    "\n",
    "    loss_dict = criterion(output, batch_gt)\n",
    "    \n",
    "    loss = args.lambda_3d      * loss_dict['loss_3d_pos']      + \\\n",
    "        args.lambda_scale   * loss_dict['loss_3d_scale']    + \\\n",
    "        args.lambda_3dv     * loss_dict['loss_3d_velocity'] + \\\n",
    "        args.lambda_lv      * loss_dict['loss_lv']          + \\\n",
    "        args.lambda_lg      * loss_dict['loss_lg']          + \\\n",
    "        args.lambda_a       * loss_dict['loss_a']           + \\\n",
    "        args.lambda_av      * loss_dict['loss_av']          + \\\n",
    "        args.lambda_shape   * loss_dict['loss_shape']       + \\\n",
    "        args.lambda_pose    * loss_dict['loss_pose']        + \\\n",
    "        args.lambda_norm    * loss_dict['loss_norm'] \n",
    "    # update metric\n",
    "    losses.update(loss.item(), batch_size)\n",
    "    loss_str = ''\n",
    "    for k, v in loss_dict.items():\n",
    "        losses_dict[k].update(v.item(), batch_size)\n",
    "        loss_str += '{0} {loss.val:.3f} ({loss.avg:.3f})\\t'.format(k, loss=losses_dict[k])\n",
    "    mpjpe, mpve = compute_error(output, batch_gt)\n",
    "    mpjpes.update(mpjpe, batch_size)\n",
    "    mpves.update(mpve, batch_size)\n",
    "    \n",
    "    for keys in output[0].keys():\n",
    "        if type(output[0][keys]) != np.ndarray: \n",
    "            output[0][keys] = output[0][keys].detach().cpu().numpy()\n",
    "        if type(batch_gt[keys]) != np.ndarray: \n",
    "            batch_gt[keys] = batch_gt[keys].detach().cpu().numpy()\n",
    "    results['kp_3d'].append(output[0]['kp_3d'])\n",
    "    results['verts'].append(output[0]['verts'])\n",
    "    results['kp_3d_gt'].append(batch_gt['kp_3d'])\n",
    "    results['verts_gt'].append(batch_gt['verts'])\n",
    "\n",
    "    # measure elapsed time\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    if idx % int(100) == 0:\n",
    "        print('Test: [{0}/{1}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "            '{2}'\n",
    "            'PVE {mpves.val:.3f} ({mpves.avg:.3f})\\t'\n",
    "            'JPE {mpjpes.val:.3f} ({mpjpes.avg:.3f})'.format(\n",
    "            idx, 1, loss_str, batch_time=batch_time,\n",
    "            loss=losses, mpves=mpves, mpjpes=mpjpes))\n",
    "           ###################################################################\n",
    "\n",
    "    print(f'==> start concating results of {dataset_name}')\n",
    "    for term in results.keys():\n",
    "        results[term] = np.concatenate(results[term])\n",
    "    print(f'==> start evaluating {dataset_name}...')\n",
    "    error_dict = evaluate_mesh(results)\n",
    "    err_str = ''\n",
    "    for err_key, err_val in error_dict.items():\n",
    "        err_str += '{}: {:.2f}mm \\t'.format(err_key, err_val)\n",
    "    print(f'=======================> {dataset_name} validation done: ', loss_str)\n",
    "    print(f'=======================> {dataset_name} validation done: ', err_str)\n",
    "    return losses.avg, error_dict['mpjpe'], error_dict['pa_mpjpe'], error_dict['mpve'], losses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 16, 17, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128, 16, 17, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========> validating 3dpw\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Pred is\n",
      "torch.Size([1, 1, 17, 3])\n",
      "GT is\n",
      "torch.Size([1, 1, 17, 3])\n",
      "Test: [0/1]\tTime 0.020 (0.020)\tLoss 341.7729 (341.7729)\tloss_3d_pos 382.523 (382.523)\tloss_3d_scale 89.297 (89.297)\tloss_3d_velocity 0.000 (0.000)\tloss_lv 0.000 (0.000)\tloss_lg 245.121 (245.121)\tloss_a 0.109 (0.109)\tloss_av 0.000 (0.000)\tloss_shape 0.302 (0.302)\tloss_pose 0.103 (0.103)\tloss_norm 2.350 (2.350)\tPVE 412.270 (412.270)\tJPE 382.523 (382.523)\n",
      "==> start concating results of 3dpw\n",
      "==> start evaluating 3dpw...\n",
      "=======================> 3dpw validation done:  loss_3d_pos 382.523 (382.523)\tloss_3d_scale 89.297 (89.297)\tloss_3d_velocity 0.000 (0.000)\tloss_lv 0.000 (0.000)\tloss_lg 245.121 (245.121)\tloss_a 0.109 (0.109)\tloss_av 0.000 (0.000)\tloss_shape 0.302 (0.302)\tloss_pose 0.103 (0.103)\tloss_norm 2.350 (2.350)\t\n",
      "=======================> 3dpw validation done:  mpve: 412.27mm \tmpjpe: 408.34mm \tpa_mpjpe: 44.19mm \tmpjpe_17j: 382.52mm \tpa_mpjpe_17j: 43.20mm \t\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mpjpe, test_pa_mpjpe, test_mpve, test_losses_dict = validate(criterion=criterion, dataset_name='3dpw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: pipeline to predict all images and then get values (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output should be a list of these\n",
    "\n",
    "sample_prediction = {\n",
    "    \"theta\": pred_thetas,\n",
    "    \"kp_3d\": pred_kp_3d,\n",
    "    \"verts\": pred_verts,\n",
    "}\n",
    "\n",
    "# Afterwards, check if they have the sae order as the GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verts = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_1\\all_thetas.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(898, 72)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_verts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATE ALL EVALUATION DICTIONARIES FOR ONE PERSON OF A SEQUENCE\n",
    "\n",
    "\n",
    "\n",
    "J_regressor_motionbert = np.load(\"Joint_regressor/J_regressor_h36m_correct.npy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_pred_thetas = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_1\\all_thetas.npy\")\n",
    "#all_pred_thetas = np.concatenate([pred_thetas, np.zeros(10)])\n",
    "all_pred_verts = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_1\\vertices_all_frames.npy\")\n",
    "#J_regressor = J_regressor.to('cpu')\n",
    "#all_pred_kp_3d = J_regressor_motionbert @ pred_verts\n",
    "\n",
    "\n",
    "\n",
    "def preds_to_dict(all_pred_thetas, all_pred_verts, J_regressor):\n",
    "    all_outputs = []\n",
    "    \n",
    "    for i in range(len(all_pred_thetas)-1):\n",
    "        pred_thetas = all_pred_thetas[i]\n",
    "        pred_verts = all_pred_verts[i]\n",
    "\n",
    "        pred_thetas = np.concatenate([pred_thetas, np.zeros(10)])\n",
    "        pred_kp_3d = J_regressor @ pred_verts\n",
    "\n",
    "\n",
    "        # Define the rotation matrix for -180 degrees around the x-axis\n",
    "        R_x = np.array([\n",
    "            [1,  0,  0],\n",
    "            [0, -1,  0],\n",
    "            [0,  0, -1]\n",
    "        ])\n",
    "\n",
    "        # Rotate each point by applying the rotation matrix\n",
    "        pred_kp_3d = pred_kp_3d @ R_x.T\n",
    "\n",
    "\n",
    "        # Extract the first coordinate\n",
    "        first_point = pred_kp_3d[0]\n",
    "\n",
    "        # Translate all points to set the first point as the origin\n",
    "        pred_kp_3d = pred_kp_3d - first_point\n",
    "\n",
    "        # Ensure the first point is now exactly zero\n",
    "        pred_kp_3d[0] = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "\n",
    "        pred_thetas = torch.tensor(pred_thetas).unsqueeze(0).unsqueeze(0)\n",
    "        pred_verts = torch.tensor(pred_verts).unsqueeze(0).unsqueeze(0)\n",
    "        pred_kp_3d = torch.tensor(pred_kp_3d).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Move tensors to GPU\n",
    "        pred_thetas = pred_thetas.to(device)\n",
    "        pred_verts = pred_verts.to(device)\n",
    "        pred_kp_3d = pred_kp_3d.to(device)\n",
    "\n",
    "\n",
    "        sample_prediction = {\n",
    "            \"theta\": pred_thetas,\n",
    "            \"kp_3d\": pred_kp_3d,\n",
    "            \"verts\": pred_verts,\n",
    "        }\n",
    "\n",
    "        all_outputs.append(sample_prediction)\n",
    "\n",
    "    return(all_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequence_outputs = preds_to_dict(all_pred_thetas, all_pred_verts, J_regressor_motionbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_thetas = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\all_thetas.npy\")\n",
    "#all_pred_thetas = np.concatenate([pred_thetas, np.zeros(10)])\n",
    "all_pred_verts = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\vertices_all_frames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895, 6890, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred_verts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOOK AT CREATION OF THETAS; ONLY SAVE THE ONES WITH COMPOSE_VALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_1\\all_thetas.npy\n",
      "E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\all_thetas.npy\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 895 is out of bounds for axis 0 with size 895",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8088\\1629658259.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;31m# Call the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds_to_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_pred_thetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_pred_verts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_regressor_motionbert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                 \u001b[0mall_sequences_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8088\\309672476.py\u001b[0m in \u001b[0;36mpreds_to_dict\u001b[1;34m(all_pred_thetas, all_pred_verts, J_regressor)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_pred_thetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mpred_thetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_pred_thetas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mpred_verts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_pred_verts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mpred_thetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred_thetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 895 is out of bounds for axis 0 with size 895"
     ]
    }
   ],
   "source": [
    "base_dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "all_sequences_outputs = []\n",
    "\n",
    "# Walk through the directory structure\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    if \"MotionBert\" in root:\n",
    "        person_folders = [os.path.join(root, d) for d in dirs if d.startswith(\"person_\")]\n",
    "        for person_folder in person_folders:\n",
    "            # Load the files\n",
    "            vertices_path = os.path.join(person_folder, \"vertices_all_frames.npy\")\n",
    "            thetas_path = os.path.join(person_folder, \"all_thetas.npy\")\n",
    "            print(thetas_path)\n",
    "            \n",
    "            if os.path.exists(vertices_path) and os.path.exists(thetas_path):\n",
    "                all_pred_verts = np.load(vertices_path)\n",
    "                all_pred_thetas = np.load(thetas_path)\n",
    "                \n",
    "                # Call the function\n",
    "                sequence_output = preds_to_dict(all_pred_thetas, all_pred_verts, J_regressor_motionbert)\n",
    "                all_sequences_outputs.append(sequence_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_1\\vertices_all_frames.npy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9808\\378500529.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;31m# Call the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0msample_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds_to_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_thetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_verts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_regressor_motionbert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                 \u001b[0mall_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9808\\1191392682.py\u001b[0m in \u001b[0;36mpreds_to_dict\u001b[1;34m(pred_thetas, pred_verts, J_regressor)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreds_to_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_thetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_verts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_regressor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpred_thetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred_thetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mpred_kp_3d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJ_regressor\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mpred_verts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Base directory\n",
    "base_dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "\n",
    "# To store all the results\n",
    "all_outputs = []\n",
    "\n",
    "# Walk through the directory structure\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    if \"MotionBert\" in root:\n",
    "        person_folders = [os.path.join(root, d) for d in dirs if d.startswith(\"person_\")]\n",
    "        for person_folder in person_folders:\n",
    "            # Load the files\n",
    "            vertices_path = os.path.join(person_folder, \"vertices_all_frames.npy\")\n",
    "            thetas_path = os.path.join(person_folder, \"all_thetas.npy\")\n",
    "            \n",
    "            if os.path.exists(vertices_path) and os.path.exists(thetas_path):\n",
    "                print(vertices_path)\n",
    "                pred_verts = np.load(vertices_path)\n",
    "                pred_thetas = np.load(thetas_path)\n",
    "                \n",
    "                # Call the function\n",
    "                sample_prediction = preds_to_dict(pred_thetas, pred_verts, J_regressor_motionbert)\n",
    "                all_outputs.append(sample_prediction)\n",
    "\n",
    "# Now `all_outputs` contains all the processed predictions\n",
    "print(f\"Processed {len(all_outputs)} outputs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHOLE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(test_loader, model, criterion, dataset_name='h36m'):\n",
    "    model.eval()\n",
    "    print(f'===========> validating {dataset_name}')\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_dict = {'loss_3d_pos': AverageMeter(), \n",
    "                   'loss_3d_scale': AverageMeter(), \n",
    "                   'loss_3d_velocity': AverageMeter(),\n",
    "                   'loss_lv': AverageMeter(), \n",
    "                   'loss_lg': AverageMeter(), \n",
    "                   'loss_a': AverageMeter(), \n",
    "                   'loss_av': AverageMeter(), \n",
    "                   'loss_pose': AverageMeter(), \n",
    "                   'loss_shape': AverageMeter(),\n",
    "                   'loss_norm': AverageMeter(),\n",
    "    }\n",
    "    mpjpes = AverageMeter()\n",
    "    mpves = AverageMeter()\n",
    "    results = defaultdict(list)\n",
    "    smpl = SMPL(args.data_root, batch_size=1).cuda()\n",
    "    J_regressor = smpl.J_regressor_h36m\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (batch_input, batch_gt) in tqdm(enumerate(test_loader)):\n",
    "            batch_size, clip_len = batch_input.shape[:2]\n",
    "            if torch.cuda.is_available():\n",
    "                batch_gt['theta'] = batch_gt['theta'].cuda().float()\n",
    "                batch_gt['kp_3d'] = batch_gt['kp_3d'].cuda().float()\n",
    "                batch_gt['verts'] = batch_gt['verts'].cuda().float()\n",
    "                batch_input = batch_input.cuda().float()\n",
    "            output = model(batch_input)    \n",
    "            output_final = output\n",
    "            if args.flip:\n",
    "                batch_input_flip = flip_data(batch_input)\n",
    "                output_flip = model(batch_input_flip)\n",
    "                output_flip_pose = output_flip[0]['theta'][:, :, :72]\n",
    "                output_flip_shape = output_flip[0]['theta'][:, :, 72:]\n",
    "                output_flip_pose = flip_thetas_batch(output_flip_pose)\n",
    "                output_flip_pose = output_flip_pose.reshape(-1, 72)\n",
    "                output_flip_shape = output_flip_shape.reshape(-1, 10)\n",
    "                output_flip_smpl = smpl(\n",
    "                    betas=output_flip_shape,\n",
    "                    body_pose=output_flip_pose[:, 3:],\n",
    "                    global_orient=output_flip_pose[:, :3],\n",
    "                    pose2rot=True\n",
    "                )\n",
    "                output_flip_verts = output_flip_smpl.vertices.detach()*1000.0\n",
    "                J_regressor_batch = J_regressor[None, :].expand(output_flip_verts.shape[0], -1, -1).to(output_flip_verts.device)\n",
    "                output_flip_kp3d = torch.matmul(J_regressor_batch, output_flip_verts)  # (NT,17,3) \n",
    "                output_flip_back = [{\n",
    "                    'theta': torch.cat((output_flip_pose.reshape(batch_size, clip_len, -1), output_flip_shape.reshape(batch_size, clip_len, -1)), dim=-1),\n",
    "                    'verts': output_flip_verts.reshape(batch_size, clip_len, -1, 3),\n",
    "                    'kp_3d': output_flip_kp3d.reshape(batch_size, clip_len, -1, 3),\n",
    "                }]\n",
    "                output_final = [{}]\n",
    "                for k, v in output_flip[0].items():\n",
    "                    output_final[0][k] = (output[0][k] + output_flip_back[0][k])*0.5\n",
    "                output = output_final\n",
    "            loss_dict = criterion(output, batch_gt)\n",
    "            loss = args.lambda_3d      * loss_dict['loss_3d_pos']      + \\\n",
    "                   args.lambda_scale   * loss_dict['loss_3d_scale']    + \\\n",
    "                   args.lambda_3dv     * loss_dict['loss_3d_velocity'] + \\\n",
    "                   args.lambda_lv      * loss_dict['loss_lv']          + \\\n",
    "                   args.lambda_lg      * loss_dict['loss_lg']          + \\\n",
    "                   args.lambda_a       * loss_dict['loss_a']           + \\\n",
    "                   args.lambda_av      * loss_dict['loss_av']          + \\\n",
    "                   args.lambda_shape   * loss_dict['loss_shape']       + \\\n",
    "                   args.lambda_pose    * loss_dict['loss_pose']        + \\\n",
    "                   args.lambda_norm    * loss_dict['loss_norm'] \n",
    "            # update metric\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            loss_str = ''\n",
    "            for k, v in loss_dict.items():\n",
    "                losses_dict[k].update(v.item(), batch_size)\n",
    "                loss_str += '{0} {loss.val:.3f} ({loss.avg:.3f})\\t'.format(k, loss=losses_dict[k])\n",
    "            mpjpe, mpve = compute_error(output, batch_gt)\n",
    "            mpjpes.update(mpjpe, batch_size)\n",
    "            mpves.update(mpve, batch_size)\n",
    "            \n",
    "            for keys in output[0].keys():\n",
    "                output[0][keys] = output[0][keys].detach().cpu().numpy()\n",
    "                batch_gt[keys] = batch_gt[keys].detach().cpu().numpy()\n",
    "            results['kp_3d'].append(output[0]['kp_3d'])\n",
    "            results['verts'].append(output[0]['verts'])\n",
    "            results['kp_3d_gt'].append(batch_gt['kp_3d'])\n",
    "            results['verts_gt'].append(batch_gt['verts'])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if idx % int(100) == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      '{2}'\n",
    "                      'PVE {mpves.val:.3f} ({mpves.avg:.3f})\\t'\n",
    "                      'JPE {mpjpes.val:.3f} ({mpjpes.avg:.3f})'.format(\n",
    "                       idx, len(test_loader), loss_str, batch_time=batch_time,\n",
    "                       loss=losses, mpves=mpves, mpjpes=mpjpes))\n",
    "\n",
    "    print(f'==> start concating results of {dataset_name}')\n",
    "    for term in results.keys():\n",
    "        results[term] = np.concatenate(results[term])\n",
    "    print(f'==> start evaluating {dataset_name}...')\n",
    "    error_dict = evaluate_mesh(results)\n",
    "    err_str = ''\n",
    "    for err_key, err_val in error_dict.items():\n",
    "        err_str += '{}: {:.2f}mm \\t'.format(err_key, err_val)\n",
    "    print(f'=======================> {dataset_name} validation done: ', loss_str)\n",
    "    print(f'=======================> {dataset_name} validation done: ', err_str)\n",
    "    return losses.avg, error_dict['mpjpe'], error_dict['pa_mpjpe'], error_dict['mpve'], losses_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
