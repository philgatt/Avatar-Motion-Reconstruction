{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALPHAPOSE IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import natsort\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "alphapose_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\"\n",
    "\n",
    "if alphapose_path  not in sys.path:\n",
    "    sys.path.append(alphapose_path)\n",
    "\n",
    "from detector.apis import get_detector\n",
    "from trackers.tracker_api import Tracker\n",
    "from trackers.tracker_cfg import cfg as tcfg\n",
    "from trackers import track\n",
    "from alphapose.models import builder\n",
    "from alphapose.utils.config import update_config\n",
    "from alphapose.utils.detector import DetectionLoader\n",
    "from alphapose.utils.file_detector import FileDetectionLoader\n",
    "from alphapose.utils.transforms import flip, flip_heatmap\n",
    "from alphapose.utils.vis import getTime\n",
    "from alphapose.utils.webcam_detector import WebCamDetectionLoader\n",
    "from alphapose.utils.writer import DataWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOTIONBERT IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "motionbert_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBERT\\MotionBERT\"\n",
    "\n",
    "if motionbert_path  not in sys.path:\n",
    "    sys.path.append(motionbert_path)\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.utils.tools import *\n",
    "from lib.utils.learning import *\n",
    "from lib.utils.utils_data import flip_data\n",
    "from lib.data.dataset_wild import WildDetDataset\n",
    "from lib.utils.vismo import render_and_save\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import errno\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from lib.utils.tools import *\n",
    "from lib.model.loss import *\n",
    "from lib.model.loss_mesh import *\n",
    "from lib.utils.utils_mesh import *\n",
    "from lib.utils.utils_smpl import *\n",
    "from lib.utils.utils_data import *\n",
    "from lib.utils.learning import *\n",
    "from lib.data.dataset_mesh import MotionSMPL\n",
    "from lib.model.model_mesh import MeshRegressor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lib.data.augmentation import Augmenter3D\n",
    "from lib.utils.tools import read_pkl\n",
    "from lib.utils.utils_data import flip_data, crop_scale\n",
    "from lib.utils.utils_mesh import flip_thetas\n",
    "from lib.utils.utils_smpl import SMPL\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lib.data.datareader_h36m import DataReaderH36M  \n",
    "from lib.data.datareader_mesh import DataReaderMesh  \n",
    "from lib.data.dataset_action import random_move  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TYPE = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT ALPHAPOSE COORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Demo options\n",
    "        self.config = 'config.yaml'  # experiment configure file name\n",
    "        self.checkpoint = 'model.pth'  # checkpoint file name\n",
    "        self.sp = False  # Use single process for pytorch\n",
    "        self.detector = \"yolo\"  # detector name\n",
    "        self.detfile = \"\"  # detection result file\n",
    "        self.inputpath = \"input_images\"  # image-directory\n",
    "        self.inputlist = \"\"  # image-list\n",
    "        self.inputimg = \"example.jpg\"  # image-name\n",
    "        self.outputpath = \"examples/res/\"  # output-directory\n",
    "        self.save_img = False  # save result as image\n",
    "        self.vis = False  # visualize image\n",
    "        self.showbox = False  # visualize human bbox\n",
    "        self.profile = False  # add speed profiling at screen output\n",
    "        self.format = None  # save in format: coco/cmu/open\n",
    "        self.min_box_area = 0  # min box area to filter out\n",
    "        self.detbatch = 5  # detection batch size PER GPU\n",
    "        self.posebatch = 64  # pose estimation maximum batch size PER GPU\n",
    "        self.eval = False  # save result json as coco format with image index\n",
    "        self.gpus = \"0\"  # CUDA devices to use, or -1 for CPU\n",
    "        self.qsize = 1024  # length of result buffer\n",
    "        self.flip = False  # enable flip testing\n",
    "        self.debug = False  # print detail information\n",
    "\n",
    "        # Video options\n",
    "        self.video = \"\"  # video-name\n",
    "        self.webcam = -1  # webcam number\n",
    "        self.save_video = False  # whether to save rendered video\n",
    "        self.vis_fast = False  # use fast rendering\n",
    "\n",
    "        # Tracking options\n",
    "        self.pose_flow = False  # track humans in video with PoseFlow\n",
    "        self.pose_track = False  # track humans in video with reid\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.cfg = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\configs\\halpe_26\\resnet\\256x192_res50_lr1e-3_1x_jupyter.yaml'\n",
    "args.checkpoint  = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\pretrained_models\\halpe26_fast_res50_256x192.pth\"\n",
    "args.video = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\downtown_bus_00\\downtown_bus_00.mp4\"\n",
    "args.outputpath = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_bus_00'\n",
    "args.save_video = False\n",
    "args.vis_fast = False\n",
    "\n",
    "cfg = update_config(args.cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_finish_info():\n",
    "    print('===========================> Finish Model Running.')\n",
    "    if (args.save_img or args.save_video) and not args.vis_fast:\n",
    "        print('===========================> Rendering remaining images in the queue...')\n",
    "        print('===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions will be done on device:\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    args.sp = True\n",
    "\n",
    "args.gpus = [int(i) for i in args.gpus.split(',')] if torch.cuda.device_count() >= 1 else [-1]\n",
    "args.device = torch.device(\"cuda:\" + str(args.gpus[0]) if args.gpus[0] >= 0 else \"cpu\")\n",
    "args.detbatch = args.detbatch * len(args.gpus)\n",
    "args.posebatch = args.posebatch * len(args.gpus)\n",
    "args.tracking = args.pose_track or args.pose_flow or args.detector=='tracker'\n",
    "\n",
    "print(\"Predictions will be done on device:\")\n",
    "print(args.device)\n",
    "\n",
    "if not args.sp:\n",
    "    torch.multiprocessing.set_start_method('forkserver', force=True)\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test folders: ['downtown_arguing_00', 'downtown_bar_00', 'downtown_bus_00', 'downtown_cafe_00', 'downtown_car_00', 'downtown_crossStreets_00', 'downtown_downstairs_00', 'downtown_enterShop_00', 'downtown_rampAndStairs_00', 'downtown_runForBus_00', 'downtown_runForBus_01', 'downtown_sitOnStairs_00', 'downtown_stairs_00', 'downtown_upstairs_00', 'downtown_walkBridge_01', 'downtown_walking_00', 'downtown_walkUphill_00', 'downtown_warmWelcome_00', 'downtown_weeklyMarket_00', 'downtown_windowShopping_00', 'flat_guitar_01', 'flat_packBags_00', 'office_phoneCall_00', 'outdoors_fencing_01']\n",
      "Train folders: ['courtyard_arguing_00', 'courtyard_backpack_00', 'courtyard_basketball_00', 'courtyard_bodyScannerMotions_00', 'courtyard_box_00', 'courtyard_capoeira_00', 'courtyard_captureSelfies_00', 'courtyard_dancing_01', 'courtyard_giveDirections_00', 'courtyard_golf_00', 'courtyard_goodNews_00', 'courtyard_jacket_00', 'courtyard_laceShoe_00', 'courtyard_rangeOfMotions_00', 'courtyard_relaxOnBench_00', 'courtyard_relaxOnBench_01', 'courtyard_shakeHands_00', 'courtyard_warmWelcome_00', 'outdoors_climbing_00', 'outdoors_climbing_01', 'outdoors_climbing_02', 'outdoors_freestyle_00', 'outdoors_slalom_00', 'outdoors_slalom_01']\n",
      "Validation folders: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Initialize lists for test, train, and validation folders\n",
    "test_folders = []\n",
    "train_folders = []\n",
    "val_folders = []\n",
    "\n",
    "# Base directory path\n",
    "base_dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\sequenceFiles\\sequenceFiles\"\n",
    "\n",
    "# Subfolder names\n",
    "subfolders = {\n",
    "    \"test\": test_folders,\n",
    "    \"train\": train_folders,\n",
    "    \"validation\": val_folders\n",
    "}\n",
    "\n",
    "# Iterate over each subfolder\n",
    "for folder_name, file_list in subfolders.items():\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if os.path.exists(folder_path):  # Check if the folder exists\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".pkl\"):\n",
    "                file_list.append(os.path.splitext(file)[0])  # Add the filename without extension\n",
    "\n",
    "# Print the lists\n",
    "print(\"Test folders:\", test_folders)\n",
    "print(\"Train folders:\", train_folders)\n",
    "print(\"Validation folders:\", val_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\"\n",
    "output_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "\n",
    "# Step 1: Traverse directories and find all the .mp4 files\n",
    "video_paths = glob.glob(os.path.join(input_base_path, \"**\", \"*.mp4\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courtyard_arguing_00\n",
      "courtyard_backpack_00\n",
      "courtyard_basketball_00\n",
      "courtyard_bodyScannerMotions_00\n",
      "courtyard_box_00\n",
      "courtyard_capoeira_00\n",
      "courtyard_captureSelfies_00\n",
      "courtyard_dancing_01\n",
      "courtyard_giveDirections_00\n",
      "courtyard_golf_00\n",
      "courtyard_goodNews_00\n",
      "courtyard_jacket_00\n",
      "courtyard_laceShoe_00\n",
      "courtyard_rangeOfMotions_00\n",
      "courtyard_relaxOnBench_00\n",
      "courtyard_relaxOnBench_01\n",
      "courtyard_shakeHands_00\n",
      "courtyard_warmWelcome_00\n",
      "outdoors_climbing_00\n",
      "outdoors_climbing_01\n",
      "outdoors_climbing_02\n",
      "outdoors_freestyle_00\n",
      "outdoors_slalom_00\n",
      "outdoors_slalom_01\n"
     ]
    }
   ],
   "source": [
    "if DATASET_TYPE == 'train':\n",
    "    video_paths = [path for path in video_paths if path.split(os.sep)[-2] in train_folders]\n",
    "    for path in video_paths:\n",
    "        print(path.split(os.sep)[-2])\n",
    "elif DATASET_TYPE == 'test':\n",
    "    video_paths = [path for path in video_paths if path.split(os.sep)[-2] in test_folders]\n",
    "elif DATASET_TYPE == 'val':\n",
    "    video_paths = [path for path in video_paths if path.split(os.sep)[-2] not in train_folders and path.split(os.sep)[-2] not in test_folders]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\"\n",
    "output_test_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "output_val_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\val_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO detector...\n",
      "{'CONFIG': 'E:/Users/Philipp/Dokumente/Pose_Estimation_3D/Alphapose/AlphaPose/detector/yolo/cfg/yolov3-spp.cfg', 'WEIGHTS': 'E:/Users/Philipp/Dokumente/Pose_Estimation_3D/Alphapose/AlphaPose/detector/yolo/data/yolov3-spp.weights', 'INP_DIM': 608, 'NMS_THRES': 0.6, 'CONFIDENCE': 0.1, 'NUM_CLASSES': 80}\n",
      "Loading pose model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda-envs\\main_pw3d\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "e:\\miniconda-envs\\main_pw3d\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_arguing_00\\courtyard_arguing_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/765 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n",
      "E:/Users/Philipp/Dokumente/Pose_Estimation_3D/Alphapose/AlphaPose/detector/yolo/cfg/yolov3-spp.cfg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 617/765 [00:36<00:05, 28.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:41<00:00, 18.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_backpack_00\\courtyard_backpack_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 618/1262 [00:26<00:25, 25.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1262/1262 [00:39<00:00, 32.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_basketball_00\\courtyard_basketball_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:19<00:00, 23.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_bodyScannerMotions_00\\courtyard_bodyScannerMotions_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 617/1257 [00:25<00:24, 25.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1257/1257 [00:38<00:00, 32.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_box_00\\courtyard_box_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 622/1041 [00:25<00:12, 33.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1041/1041 [00:32<00:00, 32.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_capoeira_00\\courtyard_capoeira_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435/435 [00:16<00:00, 26.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_captureSelfies_00\\courtyard_captureSelfies_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 624/750 [00:24<00:02, 55.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:26<00:00, 28.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue....\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_dancing_01\\courtyard_dancing_01.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:12<00:00, 28.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_giveDirections_00\\courtyard_giveDirections_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 620/848 [00:27<00:04, 50.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 848/848 [00:32<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue....\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_golf_00\\courtyard_golf_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [00:19<00:00, 31.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_goodNews_00\\courtyard_goodNews_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 431/431 [00:15<00:00, 27.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_jacket_00\\courtyard_jacket_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 617/1245 [00:25<00:24, 25.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1245/1245 [00:38<00:00, 32.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_laceShoe_00\\courtyard_laceShoe_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 629/931 [00:24<00:04, 68.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 931/931 [00:29<00:00, 31.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_rangeOfMotions_00\\courtyard_rangeOfMotions_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:26<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_relaxOnBench_00\\courtyard_relaxOnBench_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 558/558 [00:17<00:00, 32.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_relaxOnBench_01\\courtyard_relaxOnBench_01.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 616/959 [00:23<00:12, 28.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 959/959 [00:30<00:00, 31.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_shakeHands_00\\courtyard_shakeHands_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:15<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\courtyard_warmWelcome_00\\courtyard_warmWelcome_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [00:23<00:00, 25.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\outdoors_climbing_00\\outdoors_climbing_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 618/1228 [00:23<00:21, 27.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:40<00:00, 30.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\outdoors_climbing_01\\outdoors_climbing_01.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 620/1062 [00:27<00:18, 23.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1062/1062 [00:34<00:00, 30.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\outdoors_climbing_02\\outdoors_climbing_02.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 387/916 [00:17<00:18, 29.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No boxes for frame 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 621/916 [00:24<00:04, 66.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 916/916 [00:29<00:00, 31.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\outdoors_freestyle_00\\outdoors_freestyle_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:16<00:00, 30.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\outdoors_slalom_00\\outdoors_slalom_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333/333 [00:10<00:00, 31.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\outdoors_slalom_01\\outdoors_slalom_01.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 371/371 [00:11<00:00, 31.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def initialize_models():\n",
    "    \"\"\"Load YOLO and Pose models once.\"\"\"\n",
    "    print(\"Loading YOLO detector...\")\n",
    "    yolo_detector = get_detector(args)  # Replace with actual detector initialization\n",
    "    print(\"Loading pose model...\")\n",
    "    pose_model = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n",
    "    pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n",
    "    if len(args.gpus) > 1:\n",
    "        pose_model = torch.nn.DataParallel(pose_model, device_ids=args.gpus).to(args.device)\n",
    "    else:\n",
    "        pose_model.to(args.device)\n",
    "    pose_model.eval()\n",
    "    print(\"Models loaded successfully.\")\n",
    "    return yolo_detector, pose_model\n",
    "\n",
    "i_ = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load models once\n",
    "    yolo_detector, pose_model = initialize_models()\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        \n",
    "        # Generate the output directory structure\n",
    "        relative_path = os.path.relpath(video_path, input_base_path)\n",
    "        cur_folder = os.path.basename(os.path.dirname(video_path))\n",
    "        if cur_folder in train_folders:\n",
    "            output_base_path = output_train_base_path\n",
    "        elif cur_folder in test_folders:\n",
    "            output_base_path = output_test_base_path\n",
    "        elif cur_folder in val_folders:\n",
    "            output_base_path = output_val_base_path\n",
    "        else:\n",
    "            print(f'Skipping folder {cur_folder}, because its set is not defined.')\n",
    "            continue\n",
    "\n",
    "        output_dir = os.path.join(output_base_path, os.path.dirname(relative_path))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Set the output path for the specific video\n",
    "        args.outputpath = output_dir\n",
    "        input_source = video_path\n",
    "        \n",
    "        mode = \"video\"  # Assume videos for this example; adapt based on your needs\n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        print(f'Saving in {output_base_path}')\n",
    "        \n",
    "        # Load detection loader for the specific video\n",
    "        det_loader = DetectionLoader(input_source, yolo_detector, cfg, args, batchSize=args.detbatch, mode=mode, queueSize=args.qsize)\n",
    "        det_worker = det_loader.start()\n",
    "        \n",
    "        runtime_profile = {\n",
    "            'dt': [],\n",
    "            'pt': [],\n",
    "            'pn': []\n",
    "        }\n",
    "        \n",
    "        # Init data writer\n",
    "        queueSize = 2 if mode == 'webcam' else args.qsize\n",
    "        if args.save_video and mode != 'image':\n",
    "            from alphapose.utils.writer import DEFAULT_VIDEO_SAVE_OPT as video_save_opt\n",
    "            video_save_opt['savepath'] = os.path.join(args.outputpath, 'AlphaPose_' + os.path.basename(input_source))\n",
    "            video_save_opt.update(det_loader.videoinfo)\n",
    "            writer = DataWriter(cfg, args, save_video=True, video_save_opt=video_save_opt, queueSize=queueSize).start()\n",
    "        else:\n",
    "            writer = DataWriter(cfg, args, save_video=False, queueSize=queueSize).start()\n",
    "        \n",
    "        data_len = det_loader.length\n",
    "        im_names_desc = tqdm(range(data_len), dynamic_ncols=True)\n",
    "\n",
    "        batchSize = args.posebatch\n",
    "        if args.flip:\n",
    "            batchSize = int(batchSize / 2)\n",
    "        try:\n",
    "            for i in im_names_desc:\n",
    "                start_time = getTime()\n",
    "                with torch.no_grad():\n",
    "                    (inps, orig_img, im_name, boxes, scores, ids, cropped_boxes) = det_loader.read()\n",
    "                    # # Create a zero-filled tensor for keypoints\n",
    "                    # # Example of printing values for debugging\n",
    "                    # print(\"Inputs:\", inps.shape)                # Print inputs\n",
    "                    # print(\"Original Image:\", orig_img.shape)    # Print the original image\n",
    "                    # print(\"Image Name:\", im_name)          # Print the image name\n",
    "                    # print(\"Bounding Boxes:\", boxes.shape)       # Print bounding boxes\n",
    "                    # print(\"Scores:\", scores.shape)               # Print scores\n",
    "                    # print(\"IDs:\", ids.shape)                     # Print object IDs\n",
    "                    # print(\"Cropped Boxes:\", cropped_boxes.shape)  # Print cropped bounding boxes\n",
    "                    if orig_img is None:\n",
    "                        break\n",
    "                    if boxes is None or boxes.nelement() == 0:\n",
    "                        print('No boxes for frame', i)\n",
    "                        # Create a zero-filled tensor for keypoints\n",
    "                        # Example of printing values for debugging\n",
    "                        # print(\"Inputs:\", inps)                # Print inputs\n",
    "                        # print(\"Original Image:\", orig_img)    # Print the original image\n",
    "                        # print(\"Image Name:\", im_name)          # Print the image name\n",
    "                        # print(\"Bounding Boxes:\", boxes)       # Print bounding boxes\n",
    "                        # print(\"Scores:\", scores)               # Print scores\n",
    "                        # print(\"IDs:\", ids)                     # Print object IDs\n",
    "                        # print(\"Cropped Boxes:\", cropped_boxes)  # Print cropped bounding boxes\n",
    "                        # inps = torch.zeros((1, 3, 256, 192))\n",
    "                        # im_name = torch.zeros((1, 3, 256, 192))\n",
    "                        # boxes = torch.zeros((1, 4))  # Shape: [1, keypoints, (x, y, confidence)]\n",
    "                        # scores = torch.zeros((1, 1))\n",
    "                        # ids = torch.zeros((1, 1))\n",
    "                        # cropped_boxes = torch.zeros((1, 4))\n",
    "                        # writer.save(boxes, scores, ids, boxes, cropped_boxes, orig_img, im_name)\n",
    "                        prev_scores = torch.zeros_like(prev_scores)\n",
    "                        writer.save(prev_boxes, prev_scores, prev_ids, prev_hm, prev_cropped_boxes, orig_img, im_name)\n",
    "                        i_.append(i)\n",
    "                        continue\n",
    "                    if args.profile:\n",
    "                        ckpt_time, det_time = getTime(start_time)\n",
    "                        runtime_profile['dt'].append(det_time)\n",
    "                    # Pose Estimation\n",
    "                    inps = inps.to(args.device)\n",
    "                    datalen = inps.size(0)\n",
    "                    leftover = 0\n",
    "                    if (datalen) % batchSize:\n",
    "                        leftover = 1\n",
    "                    num_batches = datalen // batchSize + leftover\n",
    "                    hm = []\n",
    "                    if i == 1276:\n",
    "                        print(\"num batches\", num_batches)\n",
    "                    for j in range(num_batches):\n",
    "                        inps_j = inps[j * batchSize:min((j + 1) * batchSize, datalen)]\n",
    "                        if args.flip:\n",
    "                            inps_j = torch.cat((inps_j, flip(inps_j)))\n",
    "                        hm_j = pose_model(inps_j)\n",
    "                        if args.flip:\n",
    "                            hm_j_flip = flip_heatmap(hm_j[int(len(hm_j) / 2):], pose_dataset.joint_pairs, shift=True)\n",
    "                            hm_j = (hm_j[0:int(len(hm_j) / 2)] + hm_j_flip) / 2\n",
    "                        hm.append(hm_j)\n",
    "                        i_.append(i)\n",
    "                    hm = torch.cat(hm)\n",
    "                    if len(hm) == 0:\n",
    "                        print('length for', im_names_desc[i], 'is zero')\n",
    "                    if args.profile:\n",
    "                        ckpt_time, pose_time = getTime(ckpt_time)\n",
    "                        runtime_profile['pt'].append(pose_time)\n",
    "                    if args.pose_track:\n",
    "                        boxes, scores, ids, hm, cropped_boxes = track(tracker, args, orig_img, inps, boxes, hm, cropped_boxes, im_name, scores)\n",
    "                    # if i == 614:                  \n",
    "                    #     print(\"Boxes:\", boxes)\n",
    "                    #     print(\"Scores:\", scores)\n",
    "                    #     print(\"IDs:\", ids)\n",
    "                    #     print(\"Heatmap:\", hm)\n",
    "                    #     print(\"Cropped Boxes:\", cropped_boxes)\n",
    "                    #     print(\"Original Image:\", orig_img)\n",
    "                    #     print(\"Image Name:\", im_name)\n",
    "                        \n",
    "                    writer.save(boxes, scores, ids, hm, cropped_boxes, orig_img, im_name)\n",
    "\n",
    "                    prev_boxes = boxes\n",
    "                    prev_scores = scores\n",
    "                    prev_ids = ids\n",
    "                    prev_hm = hm\n",
    "                    prev_cropped_boxes = cropped_boxes\n",
    "                    if args.profile:\n",
    "                        ckpt_time, post_time = getTime(ckpt_time)\n",
    "                        runtime_profile['pn'].append(post_time)\n",
    "                \n",
    "                if args.profile:\n",
    "                    # TQDM\n",
    "                    im_names_desc.set_description(\n",
    "                        'det time: {dt:.4f} | pose time: {pt:.4f} | post processing: {pn:.4f}'.format(\n",
    "                            dt=np.mean(runtime_profile['dt']), pt=np.mean(runtime_profile['pt']), pn=np.mean(runtime_profile['pn']))\n",
    "                    )\n",
    "                    \n",
    "            print_finish_info()\n",
    "            while(writer.running()):\n",
    "                time.sleep(1)\n",
    "                print('===========================> Rendering remaining ' + str(writer.count()) + ' images in the queue...', end='\\r')\n",
    "            writer.stop()\n",
    "            det_loader.stop()\n",
    "        except Exception as e:\n",
    "            print(repr(e))\n",
    "            print('An error as above occurs when processing the images, please check it')\n",
    "            pass\n",
    "        except KeyboardInterrupt:\n",
    "            print_finish_info()\n",
    "            if args.sp:\n",
    "                det_loader.terminate()\n",
    "                while(writer.running()):\n",
    "                    time.sleep(1)\n",
    "                    print('===========================> Rendering remaining ' + str(writer.count()) + ' images in the queue...', end='\\r')\n",
    "                writer.stop()\n",
    "            else:\n",
    "                det_loader.terminate()\n",
    "                writer.terminate()\n",
    "                writer.clear_queues()\n",
    "                det_loader.clear_queues()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT JSON FILES INTO PERSON 1 AND PERSON 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedpw_train = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\preprocessed_train\\3dpw_train.npz\", allow_pickle=True) # preprocessed by SPIN\n",
    "threedpw_test = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\preprocessed_test\\3dpw_test.npz\", allow_pickle=True) # preprocessed by SPIN\n",
    "threedpw_val = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\preprocessed_val\\3dpw_validation.npz\", allow_pickle=True) # preprocessed by SPIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBert\\MotionBERT\\data\\mesh\\mesh_det_pw3d.pkl\", 'rb') as f:\n",
    "    dataset_pw3d = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_keypoints_to_bbox(keypoint_sets, true_bbox):\n",
    "    \"\"\"\n",
    "    Find the keypoint set whose minimal bounding box is closest to the true bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        keypoint_sets (list): List of keypoint sets, where each set is in the format [x1, y1, c1, ...].\n",
    "        true_bbox (tuple): Ground truth bounding box in the format (x_min, y_min, x_max, y_max).\n",
    "\n",
    "    Returns:\n",
    "        list: Keypoint set with the closest bounding box.\n",
    "    \"\"\"\n",
    "    def calculate_bbox(keypoints):\n",
    "        coords = [(keypoints[i], keypoints[i + 1]) for i in range(0, len(keypoints), 3)]\n",
    "        x_coords, y_coords = zip(*coords)\n",
    "        return min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
    "\n",
    "    def bbox_center(bbox):\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        return ((x_min + x_max) / 2, (y_min + y_max) / 2)\n",
    "\n",
    "    true_center = bbox_center(true_bbox)\n",
    "    \n",
    "    closest_keypoints = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    for keypoints in keypoint_sets:\n",
    "        bbox = calculate_bbox(keypoints)\n",
    "        center = bbox_center(bbox)\n",
    "        distance = np.sqrt((center[0] - true_center[0])**2 + (center[1] - true_center[1])**2)\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_keypoints = keypoints\n",
    "\n",
    "    return closest_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_bounding_box(keypoints):\n",
    "    \"\"\"\n",
    "    Calculate the minimal bounding box that encapsulates the given keypoints.\n",
    "    \n",
    "    Parameters:\n",
    "    - keypoints: A numpy array of shape (N, 2), where N is the number of keypoints.\n",
    "    \n",
    "    Returns:\n",
    "    - bbox: A tuple containing the coordinates of the bounding box (min_x, min_y, max_x, max_y).\n",
    "    \"\"\"\n",
    "    # Ensure keypoints is a numpy array\n",
    "    import numpy as np\n",
    "    keypoints = np.array(keypoints)\n",
    "    \n",
    "    # Get min and max for x and y coordinates\n",
    "    min_x = np.min(keypoints[:, 0])\n",
    "    min_y = np.min(keypoints[:, 1])\n",
    "    max_x = np.max(keypoints[:, 0])\n",
    "    max_y = np.max(keypoints[:, 1])\n",
    "    \n",
    "    # Bounding box coordinates: (min_x, min_y, max_x, max_y)\n",
    "    bbox = (min_x, min_y, max_x, max_y)\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_annot(filtered_data_p0, filtered_data_p1, folder_path):\n",
    "    # Save filtered_data_p0 as 'person1.json', overwriting if it exists\n",
    "    if filtered_data_p0 is not None:\n",
    "        person1_path = os.path.join(folder_path, 'person_1.json')\n",
    "        with open(person1_path, 'w') as json_file:\n",
    "            json.dump(filtered_data_p0, json_file, indent=4)\n",
    "        print(f\"Saved person1 data to {person1_path}\")\n",
    "\n",
    "    # Save filtered_data_p1 as 'person2.json', overwriting if it exists\n",
    "    if filtered_data_p1 is not None:\n",
    "        person2_path = os.path.join(folder_path, 'person_2.json')\n",
    "        with open(person2_path, 'w') as json_file:\n",
    "            json.dump(filtered_data_p1, json_file, indent=4)\n",
    "        print(f\"Saved person2 data to {person2_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(data, true_bboxes):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Group dictionaries by image_id\n",
    "    grouped_data = defaultdict(list)\n",
    "    for entry in data:\n",
    "        grouped_data[entry['image_id']].append(entry)\n",
    "\n",
    "    # Prepare the filtered data list\n",
    "    filtered_data = []\n",
    "\n",
    "    # Process each group\n",
    "    for idx, (image_id, entries) in enumerate(grouped_data.items()):\n",
    "        true_bbox = true_bboxes[idx]  # Get the corresponding true bbox for the frame\n",
    "        \n",
    "        if len(entries) == 1:\n",
    "            # If there's only one entry for this image_id, add it directly\n",
    "            filtered_data.append(entries[0])\n",
    "        else:\n",
    "            # Extract keypoints and apply closest_keypoints_to_bbox\n",
    "            keypoint_sets = [entry['keypoints'] for entry in entries]\n",
    "            closest_keypoints = closest_keypoints_to_bbox(keypoint_sets, true_bbox)\n",
    "\n",
    "            # Find the entry with the matching keypoints and add it\n",
    "            for entry in entries:\n",
    "                if entry['keypoints'] == closest_keypoints:\n",
    "                    filtered_data.append(entry)\n",
    "                    break\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(idx):\n",
    "\n",
    "    string = dataset_pw3d['test']['source'][idx]\n",
    "\n",
    "    # Person index\n",
    "    last_digit = string[-1]\n",
    "\n",
    "    # File index\n",
    "    second_digit = string[-2]\n",
    "\n",
    "    new_string = string[:-3] + f'0{second_digit}'\n",
    "\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names = []\n",
    "\n",
    "\n",
    "# for idx, elem in enumerate(dataset_pw3d['test']['source']):\n",
    "#     file_names.append(get_file_name(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_file_names = list(dict.fromkeys(file_names))\n",
    "# switch = unique_file_names[15]\n",
    "# unique_file_names[15] = unique_file_names[16]\n",
    "# unique_file_names[16] = switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "if DATASET_TYPE == 'train':\n",
    "    dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\"\n",
    "if DATASET_TYPE == 'test':\n",
    "    dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "if DATASET_TYPE == 'val':\n",
    "    dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\val_set\"\n",
    "\n",
    "# Get a list of folder names in the specified directory\n",
    "unique_file_names = [name for name in os.listdir(dir) if os.path.isdir(os.path.join(dir, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bboxes = []\n",
    "\n",
    "if DATASET_TYPE == 'test':\n",
    "    cur_dataset = threedpw_test\n",
    "elif DATASET_TYPE == 'train':\n",
    "    cur_dataset = threedpw_train\n",
    "elif DATASET_TYPE == 'val':\n",
    "    cur_dataset = threedpw_val\n",
    "\n",
    "\n",
    "for file_idx, file_data in enumerate(cur_dataset['all_poses_2d']):\n",
    "    file_bboxes = []\n",
    "\n",
    "    for person_idx, person_data in enumerate(file_data):  # Adjust based on structure\n",
    "        person_boxes = []\n",
    "\n",
    "        for frame in person_data:  # Iterate over frames\n",
    "            poses_2d = frame.T\n",
    "            bbox = calculate_min_bounding_box(poses_2d)\n",
    "            \n",
    "            # If bbox is None, replace it with (0, 0, 0, 0)\n",
    "            if bbox is None:\n",
    "                bbox = (0, 0, 0, 0)\n",
    "\n",
    "            person_boxes.append(bbox)\n",
    "\n",
    "        file_bboxes.append(person_boxes)\n",
    "\n",
    "    all_bboxes.append(file_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_arguing_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_arguing_00\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_arguing_00\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_arguing_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_arguing_00\\AlphaPose_courtyard_arguing_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_backpack_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_backpack_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_backpack_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_backpack_00\\AlphaPose_courtyard_backpack_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_basketball_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_basketball_00\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_basketball_00\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_basketball_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_basketball_00\\AlphaPose_courtyard_basketball_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_bodyScannerMotions_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_bodyScannerMotions_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_bodyScannerMotions_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_bodyScannerMotions_00\\AlphaPose_courtyard_bodyScannerMotions_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_box_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_box_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_box_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_box_00\\AlphaPose_courtyard_box_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_capoeira_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_capoeira_00\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_capoeira_00\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_capoeira_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_capoeira_00\\AlphaPose_courtyard_capoeira_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_captureSelfies_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_captureSelfies_00\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_captureSelfies_00\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_captureSelfies_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_captureSelfies_00\\AlphaPose_courtyard_captureSelfies_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_dancing_01\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_dancing_01\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_dancing_01\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_dancing_01\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_dancing_01\\AlphaPose_courtyard_dancing_01.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_giveDirections_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_giveDirections_00\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_giveDirections_00\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_giveDirections_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_giveDirections_00\\AlphaPose_courtyard_giveDirections_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_golf_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_golf_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_golf_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_golf_00\\AlphaPose_courtyard_golf_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_goodNews_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_goodNews_00\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_goodNews_00\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_goodNews_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_goodNews_00\\AlphaPose_courtyard_goodNews_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_jacket_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_jacket_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_jacket_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_jacket_00\\AlphaPose_courtyard_jacket_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_laceShoe_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_laceShoe_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_laceShoe_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_laceShoe_00\\AlphaPose_courtyard_laceShoe_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_rangeOfMotions_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_rangeOfMotions_00\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_rangeOfMotions_00\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_rangeOfMotions_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_rangeOfMotions_00\\AlphaPose_courtyard_rangeOfMotions_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_relaxOnBench_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_relaxOnBench_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_relaxOnBench_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_relaxOnBench_00\\AlphaPose_courtyard_relaxOnBench_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_relaxOnBench_01\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_relaxOnBench_01\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_relaxOnBench_01\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_relaxOnBench_01\\AlphaPose_courtyard_relaxOnBench_01.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_shakeHands_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_shakeHands_00\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_shakeHands_00\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_shakeHands_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_shakeHands_00\\AlphaPose_courtyard_shakeHands_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_warmWelcome_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_warmWelcome_00\\person_1.json\n",
      "Saved person2 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_warmWelcome_00\\person_2.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_warmWelcome_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\courtyard_warmWelcome_00\\AlphaPose_courtyard_warmWelcome_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_00\\AlphaPose_outdoors_climbing_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_01\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_01\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_01\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_01\\AlphaPose_outdoors_climbing_01.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_02\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_02\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_02\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_climbing_02\\AlphaPose_outdoors_climbing_02.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_freestyle_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_freestyle_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_freestyle_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_freestyle_00\\AlphaPose_outdoors_freestyle_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_slalom_00\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_slalom_00\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_slalom_00\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_slalom_00\\AlphaPose_outdoors_slalom_00.mp4\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_slalom_01\\alphapose-results.json\n",
      "Saved person1 data to E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_slalom_01\\person_1.json\n",
      "Successfully processed and saved: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_slalom_01\\alphapose-results.json\n",
      "Processing file: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\\outdoors_slalom_01\\AlphaPose_outdoors_slalom_01.mp4\n"
     ]
    }
   ],
   "source": [
    "# FIX FOLDER INDEX\n",
    "# ALWAYS SAVING BOTH PERSONS LEADS TO THE FACT THAT IF THERE IS ONLY ! PERSON; THE PREVIOUS SECOND PERSON IS SAVED\n",
    "\n",
    "if DATASET_TYPE == 'train':\n",
    "    root_dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\"\n",
    "elif DATASET_TYPE == 'test':\n",
    "    root_dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "elif DATASET_TYPE == 'val':\n",
    "    root_dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\val_set\"\n",
    "\n",
    "\n",
    "# Loop through subfolders\n",
    "# Loop through subfolders\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    # print(f\"Current directory: {root}\")\n",
    "    # print(f\"Subdirectories: {dirs}\")\n",
    "    # print(f\"Files: {files}\")\n",
    "    filtered_data_p0 = None\n",
    "    filtered_data_p1 = None\n",
    "    current_path = os.path.basename(root)\n",
    "    for file in files:\n",
    "        filtered_data_p0 = None\n",
    "        filtered_data_p1 = None\n",
    "        file_path = os.path.join(root, file)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Check if the file is a JSON file\n",
    "        if file.endswith('alphapose-results.json'):\n",
    "            json_path = os.path.join(root, file)\n",
    "            try:\n",
    "                # Open and load the JSON file\n",
    "                with open(json_path, 'r') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "\n",
    "                folder_index = unique_file_names.index(current_path)\n",
    "                # Use folder_index to access corresponding bounding boxes\n",
    "                filtered_data_p0 = filter_duplicates(data, all_bboxes[folder_index][0])\n",
    "                if len(all_bboxes[folder_index]) >= 2:\n",
    "                    filtered_data_p1 = filter_duplicates(data, all_bboxes[folder_index][1])\n",
    "\n",
    "                # Uncomment and define save_annot to save the filtered data\n",
    "                save_annot(filtered_data_p0, filtered_data_p1, os.path.join(root))\n",
    "\n",
    "\n",
    "                filtered_data_p0 = None\n",
    "                filtered_data_p1 = None\n",
    "                print(f\"Successfully processed and saved: {json_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {json_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOTIONBERT DETECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Define all your arguments as class attributes with default values\n",
    "        self.config = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBert\\MotionBERT\\configs\\pose3d\\MB_ft_h36m_global_lite.yaml\"  # Path to the config file\n",
    "        self.evaluate = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBert\\MotionBERT\\checkpoint\\pose3d\\FT_MB_lite_MB_ft_h36m_global_lite\\best_epoch.bin\"  # Checkpoint to evaluate\n",
    "        self.json_path = None  # Alphapose detection result JSON path\n",
    "        self.vid_path = None  # Video path\n",
    "        self.out_path = None  # Output path\n",
    "        self.pixel = False  # Align with pixel coordinates\n",
    "        self.focus = None  # Target person ID\n",
    "        self.clip_len = 243  # Clip length for network input\n",
    "\n",
    "\n",
    "opts = Args()\n",
    "args = get_config(opts.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "model_backbone = load_backbone(args)\n",
    "if torch.cuda.is_available():\n",
    "    model_backbone = nn.DataParallel(model_backbone)\n",
    "    model_backbone = model_backbone.cuda()\n",
    "\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load(opts.evaluate, map_location=lambda storage, loc: storage)\n",
    "model_backbone.load_state_dict(checkpoint['model_pos'], strict=True)\n",
    "model_pos = model_backbone\n",
    "model_pos.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(opts):\n",
    "\n",
    "    testloader_params = {\n",
    "        'batch_size': 1,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 8,\n",
    "        'pin_memory': True,\n",
    "        'prefetch_factor': 4,\n",
    "        'persistent_workers': True,\n",
    "        'drop_last': False\n",
    "    }\n",
    "\n",
    "    vid = imageio.get_reader(opts.vid_path, 'ffmpeg')\n",
    "    fps_in = vid.get_meta_data()['fps']\n",
    "    vid_size = vid.get_meta_data()['size']\n",
    "    os.makedirs(opts.out_path, exist_ok=True)\n",
    "\n",
    "    if opts.pixel:\n",
    "        wild_dataset = WildDetDataset(opts.json_path, clip_len=opts.clip_len, vid_size=vid_size, scale_range=None, focus=opts.focus)\n",
    "    else:\n",
    "        wild_dataset = WildDetDataset(opts.json_path, clip_len=opts.clip_len, scale_range=[1, 1], focus=opts.focus)\n",
    "\n",
    "    test_loader = DataLoader(wild_dataset, **testloader_params)\n",
    "\n",
    "    results_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch_input in tqdm(test_loader):\n",
    "            N, T = batch_input.shape[:2]\n",
    "            if torch.cuda.is_available():\n",
    "                batch_input = batch_input.cuda()\n",
    "            if args.no_conf:\n",
    "                batch_input = batch_input[:, :, :, :2]\n",
    "            if args.flip:\n",
    "                batch_input_flip = flip_data(batch_input)\n",
    "                predicted_3d_pos_1 = model_pos(batch_input)\n",
    "                predicted_3d_pos_flip = model_pos(batch_input_flip)\n",
    "                predicted_3d_pos_2 = flip_data(predicted_3d_pos_flip)\n",
    "                predicted_3d_pos = (predicted_3d_pos_1 + predicted_3d_pos_2) / 2.0\n",
    "            else:\n",
    "                predicted_3d_pos = model_pos(batch_input)\n",
    "            if args.rootrel:\n",
    "                predicted_3d_pos[:, :, 0, :] = 0\n",
    "            else:\n",
    "                predicted_3d_pos[:, 0, 0, 2] = 0\n",
    "            if args.gt_2d:\n",
    "                predicted_3d_pos[..., :2] = batch_input[..., :2]\n",
    "            results_all.append(predicted_3d_pos.cpu().numpy())\n",
    "\n",
    "    results_all = np.hstack(results_all)\n",
    "    results_all = np.concatenate(results_all)\n",
    "    #render_and_save(results_all, f'{opts.out_path}/X3D.mp4', keep_imgs=False, fps=fps_in)\n",
    "    if opts.pixel:\n",
    "        results_all = results_all * (min(vid_size) / 2.0)\n",
    "        results_all[:, :, :2] = results_all[:, :, :2] + np.array(vid_size) / 2.0\n",
    "    np.save(f'{opts.out_path}/X3D.npy', results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_TYPE == 'train':\n",
    "    current_folders = train_folders\n",
    "elif DATASET_TYPE == 'test':\n",
    "    current_folders = test_folders\n",
    "elif DATASET_TYPE == 'val':\n",
    "    current_folders = val_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_arguing_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:14<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_arguing_00, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_backpack_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:13<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_basketball_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_basketball_00, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_bodyScannerMotions_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:13<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_box_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:13<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_capoeira_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_capoeira_00, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_captureSelfies_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_captureSelfies_00, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_dancing_01, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_dancing_01, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_giveDirections_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_giveDirections_00, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_golf_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:12<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_goodNews_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_goodNews_00, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_jacket_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:12<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_laceShoe_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_rangeOfMotions_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:12<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_rangeOfMotions_00, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:12<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_relaxOnBench_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:12<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_relaxOnBench_01, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_shakeHands_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_shakeHands_00, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_warmWelcome_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:12<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: courtyard_warmWelcome_00, person: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: outdoors_climbing_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:13<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: outdoors_climbing_01, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:13<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: outdoors_climbing_02, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: outdoors_freestyle_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: outdoors_slalom_00, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: outdoors_slalom_01, person: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "# Paths\n",
    "video_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\"\n",
    "\n",
    "if DATASET_TYPE == 'train':\n",
    "    json_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\"\n",
    "    output_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\"\n",
    "\n",
    "if DATASET_TYPE == 'test':\n",
    "    json_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "    output_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "\n",
    "if DATASET_TYPE == 'val':\n",
    "    json_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\val_set\"\n",
    "    output_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\val_set\"\n",
    "\n",
    "overwrite = True  # Set to True if you want to force overwriting existing files\n",
    "\n",
    "# Iterate through each folder in the video base path\n",
    "for folder_name in os.listdir(video_base_path):\n",
    "    if folder_name in current_folders:\n",
    "        folder_path = os.path.join(video_base_path, folder_name)\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "            video_file = os.path.join(folder_path, f\"{folder_name}.mp4\")\n",
    "\n",
    "            if os.path.exists(video_file):\n",
    "                json_folder = os.path.join(json_base_path, folder_name)\n",
    "\n",
    "                if os.path.isdir(json_folder):\n",
    "                    json_files = glob.glob(os.path.join(json_folder, \"person_*.json\"))\n",
    "\n",
    "                    for json_file in json_files:\n",
    "                        person_id = os.path.splitext(os.path.basename(json_file))[0].split('_')[-1]\n",
    "                        output_path = os.path.join(output_base_path, folder_name, \"MotionBert\", f\"person_{person_id}\")\n",
    "\n",
    "                        x3d_mp4 = os.path.join(output_path, \"X3D.mp4\")\n",
    "                        x3d_npy = os.path.join(output_path, \"X3D.npy\")\n",
    "\n",
    "                        if overwrite or not (os.path.exists(x3d_mp4) and os.path.exists(x3d_npy)):\n",
    "                            print(f\"Processing folder: {folder_name}, person: {person_id}\")\n",
    "                            \n",
    "                            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "                            opts.vid_path = video_file\n",
    "                            opts.json_path = json_file\n",
    "                            opts.out_path = output_path\n",
    "                            main(opts)\n",
    "\n",
    "                            # Free up memory after processing\n",
    "                        else:\n",
    "                            print(f\"Skipping {output_path}, output files already exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATING SMPL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_params_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Pose_to_SMPL\\video2bvh\"\n",
    "if smpl_params_path not in sys.path:\n",
    "    sys.path .append(smpl_params_path)\n",
    "\n",
    "\n",
    "\n",
    "# from pose_estimator_2d import openpose_estimator\n",
    "# from pose_estimator_3d import estimator_3d\n",
    "from smpl_utils import smooth, vis, camera\n",
    "from bvh_skeleton import openpose_skeleton, h36m_skeleton, cmu_skeleton\n",
    "\n",
    "import cv2\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "#import mathutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = [(0, None), (4, None), (1, None), (7, None), (5, None), (2, None), (3, None), (6, None), (12, None), (8, None), (15, None), (None, [0, 0, 0]), (9, None), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (10, None), (13, None), (11, None), (14, None), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "h36m_skel = h36m_skeleton.H36mSkeleton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_to_quaternion_zxy(euler_z, euler_x, euler_y):\n",
    "    # Create a Rotation object with 'ZXY' order\n",
    "    r = R.from_euler('zxy', [euler_z, euler_x, euler_y], degrees=True)\n",
    "\n",
    "    # Convert to quaternion\n",
    "    quaternion = r.as_quat()  # Returns (x, y, z, w)\n",
    "\n",
    "    # Return as a tuple in the desired order (W, Y, Z, X)\n",
    "    return (quaternion[3], quaternion[1], quaternion[2], quaternion[0])\n",
    "\n",
    "\n",
    "def rodrigues_from_pose(frame, joint_name, all_rotations):\n",
    "    # Extract quaternion from input dictionary\n",
    "    quat = all_rotations[frame][joint_name]  # Assumes quaternion is in (W, X, Y, Z) order\n",
    "\n",
    "    # Create a SciPy Rotation object\n",
    "    rotation = R.from_quat([quat[1], quat[2], quat[3], quat[0]])  # Convert to (X, Y, Z, W)\n",
    "\n",
    "    # Convert to axis-angle representation\n",
    "    axis = rotation.as_rotvec()  # Returns Rodrigues vector directly (axis * angle)\n",
    "\n",
    "    return axis\n",
    "\n",
    "\n",
    "def apply_mapping(B, mapping, group_size=3):\n",
    "    \"\"\"\n",
    "    Apply a mapping to reorder and pad a list like B.\n",
    "    \"\"\"\n",
    "    # Divide B into groups of group_size\n",
    "    grouped_B = [B[i:i + group_size] for i in range(0, len(B), group_size)]\n",
    "\n",
    "    # Apply mapping\n",
    "    output_groups = []\n",
    "    for index, padding in mapping:\n",
    "        if index is not None:\n",
    "            output_groups.append(grouped_B[index])  # Reorder from B\n",
    "        elif padding is not None:\n",
    "            output_groups.append(padding)  # Add padding group\n",
    "\n",
    "    # Flatten the output groups\n",
    "    reordered_B = [item for group in output_groups for item in group]\n",
    "    return reordered_B\n",
    "\n",
    "\n",
    "def load_and_prepare_data(data):\n",
    "\n",
    "    # Load the 3D pose data from the .npy file\n",
    "    pose_data = data\n",
    "\n",
    "    # Get the number of frames and joints\n",
    "    n_frames, n_joints, _ = pose_data.shape\n",
    "\n",
    "    # Define the rotation matrix for -90 degrees around the x-axis\n",
    "    # rotation_matrix = np.array([\n",
    "    #     [1,  0,  0],\n",
    "    #     [0,  0, 1],\n",
    "    #     [0,  -1,  0]\n",
    "    # ])\n",
    "\n",
    "    rotation_matrix = np.array([\n",
    "        [1,  0,  0],\n",
    "        [0,  0, 1],\n",
    "        [0,  -1,  0]\n",
    "    ])\n",
    "\n",
    "    # Apply the rotation matrix to every frame and every joint\n",
    "    # We reshape the pose_data to (n_frames * n_joints, 3), apply the rotation, then reshape back\n",
    "    data = np.dot(pose_data.reshape(-1, 3), rotation_matrix.T).reshape(n_frames, n_joints, 3)\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def calculate_translations(channels):\n",
    "\n",
    "    all_translations = []\n",
    "\n",
    "    for index in range(len(channels)):\n",
    "        # Initialize the dictionary to hold the results\n",
    "        # Iterate through the list and extract translation values\n",
    "        translation_values = channels[index][0:3]  # Extracting the translation values\n",
    "\n",
    "        all_translations.append(translation_values)\n",
    "\n",
    "    return all_translations\n",
    "\n",
    "\n",
    "def calculate_rotations(channels, body_parts):\n",
    "\n",
    "    all_rotations = []\n",
    "\n",
    "    for index in range(len(channels)):\n",
    "        # Initialize the dictionary to hold the results\n",
    "        rotation_dict = {}\n",
    "        # Iterate through the list and extract rotation values\n",
    "        rotation_values = channels[index][3:]  # Extracting the rotation values, skipping the first two\n",
    "        # if index == 0:\n",
    "        #     print(rotation_values)\n",
    "        for i, body_part in enumerate(body_parts):\n",
    "            # Take 3 consecutive values (x, y, z rotations) for each body part\n",
    "            quat = euler_to_quaternion_zxy(-rotation_values[i*3+2], rotation_values[i*3+1], rotation_values[i*3])\n",
    "            rotation_dict[body_part] = quat\n",
    "\n",
    "        all_rotations.append(rotation_dict)\n",
    "\n",
    "    return all_rotations\n",
    "\n",
    "def calculate_all_poses(all_rotations, body_parts):\n",
    "\n",
    "    joint_names = body_parts\n",
    "    num_joints = len(joint_names)\n",
    "    frame = 0\n",
    "    all_poses = []\n",
    "\n",
    "\n",
    "    for frame in range(len(all_rotations)):\n",
    "        # Get armature pose in rodrigues representation\n",
    "        pose = [0.0] * (num_joints * 3)\n",
    "\n",
    "        for index in range(num_joints):\n",
    "            joint_name = joint_names[index]\n",
    "            joint_pose = rodrigues_from_pose(frame, joint_name, all_rotations)\n",
    "            pose[index*3 + 0] = joint_pose[2]\n",
    "            pose[index*3 + 1] = joint_pose[0]\n",
    "            pose[index*3 + 2] = joint_pose[1]\n",
    "        \n",
    "        all_poses.append(pose)\n",
    "\n",
    "    return all_poses\n",
    "\n",
    "def calculate_all_thetas(all_poses):\n",
    "    all_thetas = []\n",
    "\n",
    "    for i, frame in enumerate(all_poses):\n",
    "        theta_calculated = all_poses[i]\n",
    "\n",
    "        reordered_theta = apply_mapping(theta_calculated, mapping)\n",
    "        smpl_h_params = reordered_theta\n",
    "\n",
    "        reordered_theta = reordered_theta[0:72]\n",
    "\n",
    "        all_thetas.append(reordered_theta)\n",
    "    return all_thetas\n",
    "\n",
    "    #np.save(\"all_thetas_backpack.npy\", all_thetas)\n",
    "\n",
    "def save_smpl(translations, thetas, vertices, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    if translations is not None:\n",
    "        np.save(os.path.join(output_folder, 'all_translations.npy'), translations)\n",
    "    if thetas is not None:\n",
    "        np.save(os.path.join(output_folder, 'all_thetas.npy'), thetas)\n",
    "    if vertices is not None:\n",
    "        np.save(os.path.join(output_folder, 'vertices.npy'), vertices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smpl_output(data):\n",
    "\n",
    "    data = load_and_prepare_data(data)\n",
    "\n",
    "    channels, header = h36m_skel.poses2bvh(data, output_file=None)\n",
    "\n",
    "    all_translations = calculate_translations(channels)\n",
    "\n",
    "    # List of body parts in the order matching the data\n",
    "    body_parts = [\n",
    "        'Hip', 'RightHip', 'RightKnee', 'RightAnkle', 'LeftHip', 'LeftKnee', 'LeftAnkle', \n",
    "        'Spine', 'Thorax', 'Neck', 'LeftShoulder', 'LeftElbow', 'LeftWrist', \n",
    "        'RightShoulder', 'RightElbow', 'RightWrist'\n",
    "    ]\n",
    "\n",
    "    all_rotations = calculate_rotations(channels, body_parts)\n",
    "\n",
    "    all_poses = calculate_all_poses(all_rotations, body_parts)\n",
    "\n",
    "\n",
    "    all_thetas = calculate_all_thetas(all_poses)\n",
    "\n",
    "    return np.asarray(all_translations), np.asarray(all_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_root = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBert\\MotionBERT\\data\\mesh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "smpl = SMPL(args.data_root, batch_size=1).double().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.vid_path = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\flat_guitar_01\\flat_guitar_01.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = imageio.get_reader(opts.vid_path,  'ffmpeg')\n",
    "fps_in = vid.get_meta_data()['fps']\n",
    "vid_size = vid.get_meta_data()['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Users\\\\Philipp\\\\Dokumente\\\\Pose_Estimation_3D\\\\Alphapose\\\\AlphaPose\\\\examples\\\\res\\\\train_set\\\\outdoors_slalom_01\\\\MotionBert\\\\person_1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts.out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory paths\n",
    "dataset_dir = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW'\n",
    "\n",
    "if DATASET_TYPE == 'train':\n",
    "    base_pred_dir = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set'\n",
    "elif DATASET_TYPE == 'test':\n",
    "    base_pred_dir = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set'\n",
    "elif DATASET_TYPE == 'val':\n",
    "    base_pred_dir = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\val_set'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST WAY OF EXTRACTING VERTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_3dpw_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\readme\"\n",
    "if smpl_3dpw_path not in sys.path:\n",
    "    sys.path .append(smpl_3dpw_path)\n",
    "\n",
    "from smpl.smpl_webuser.serialization import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "motionbert_preds = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\X3D.npy\")\n",
    "predicted_translation, predicted_poses = calculate_smpl_output(motionbert_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_thetas = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\all_thetas.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_translation = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\all_translations.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequence: courtyard_arguing_00\n",
      "Loaded poses for person_1, shape: (765, 72)\n",
      "valid poses shape (765, 72)\n",
      "Loaded poses for person_2, shape: (765, 72)\n",
      "valid poses shape (765, 72)\n",
      "Processing sequence: courtyard_backpack_00\n",
      "Loaded poses for person_1, shape: (1262, 72)\n",
      "valid poses shape (1262, 72)\n",
      "Processing sequence: courtyard_basketball_00\n",
      "Loaded poses for person_1, shape: (468, 72)\n",
      "valid poses shape (418, 72)\n",
      "Loaded poses for person_2, shape: (468, 72)\n",
      "valid poses shape (468, 72)\n",
      "Processing sequence: courtyard_bodyScannerMotions_00\n",
      "Loaded poses for person_1, shape: (1257, 72)\n",
      "valid poses shape (1257, 72)\n",
      "Processing sequence: courtyard_box_00\n",
      "Loaded poses for person_1, shape: (1041, 72)\n",
      "valid poses shape (1041, 72)\n",
      "Processing sequence: courtyard_capoeira_00\n",
      "Loaded poses for person_1, shape: (435, 72)\n",
      "valid poses shape (388, 72)\n",
      "Loaded poses for person_2, shape: (435, 72)\n",
      "valid poses shape (435, 72)\n",
      "Processing sequence: courtyard_captureSelfies_00\n",
      "Loaded poses for person_1, shape: (750, 72)\n",
      "valid poses shape (677, 72)\n",
      "Loaded poses for person_2, shape: (750, 72)\n",
      "valid poses shape (693, 72)\n",
      "Processing sequence: courtyard_dancing_01\n",
      "Loaded poses for person_1, shape: (351, 72)\n",
      "valid poses shape (309, 72)\n",
      "Loaded poses for person_2, shape: (351, 72)\n",
      "valid poses shape (331, 72)\n",
      "Processing sequence: courtyard_giveDirections_00\n",
      "Loaded poses for person_1, shape: (848, 72)\n",
      "valid poses shape (848, 72)\n",
      "Loaded poses for person_2, shape: (848, 72)\n",
      "valid poses shape (848, 72)\n",
      "Processing sequence: courtyard_golf_00\n",
      "Loaded poses for person_1, shape: (604, 72)\n",
      "valid poses shape (604, 72)\n",
      "Processing sequence: courtyard_goodNews_00\n",
      "Loaded poses for person_1, shape: (431, 72)\n",
      "valid poses shape (431, 72)\n",
      "Loaded poses for person_2, shape: (431, 72)\n",
      "valid poses shape (431, 72)\n",
      "Processing sequence: courtyard_jacket_00\n",
      "Loaded poses for person_1, shape: (1245, 72)\n",
      "valid poses shape (1236, 72)\n",
      "Processing sequence: courtyard_laceShoe_00\n",
      "Loaded poses for person_1, shape: (931, 72)\n",
      "valid poses shape (931, 72)\n",
      "Processing sequence: courtyard_rangeOfMotions_00\n",
      "Loaded poses for person_1, shape: (601, 72)\n",
      "valid poses shape (594, 72)\n",
      "Loaded poses for person_2, shape: (601, 72)\n",
      "valid poses shape (601, 72)\n",
      "Processing sequence: courtyard_relaxOnBench_00\n",
      "Loaded poses for person_1, shape: (558, 72)\n",
      "valid poses shape (546, 72)\n",
      "Processing sequence: courtyard_relaxOnBench_01\n",
      "Loaded poses for person_1, shape: (959, 72)\n",
      "valid poses shape (842, 72)\n",
      "Processing sequence: courtyard_shakeHands_00\n",
      "Loaded poses for person_1, shape: (391, 72)\n",
      "valid poses shape (361, 72)\n",
      "Loaded poses for person_2, shape: (391, 72)\n",
      "valid poses shape (391, 72)\n",
      "Processing sequence: courtyard_warmWelcome_00\n",
      "Loaded poses for person_1, shape: (599, 72)\n",
      "valid poses shape (562, 72)\n",
      "Loaded poses for person_2, shape: (599, 72)\n",
      "valid poses shape (346, 72)\n",
      "Processing sequence: outdoors_climbing_00\n",
      "Loaded poses for person_1, shape: (1228, 72)\n",
      "valid poses shape (1224, 72)\n",
      "Processing sequence: outdoors_climbing_01\n",
      "Loaded poses for person_1, shape: (1062, 72)\n",
      "valid poses shape (1061, 72)\n",
      "Processing sequence: outdoors_climbing_02\n",
      "Loaded poses for person_1, shape: (916, 72)\n",
      "valid poses shape (874, 72)\n",
      "Processing sequence: outdoors_freestyle_00\n",
      "Loaded poses for person_1, shape: (498, 72)\n",
      "valid poses shape (498, 72)\n",
      "Processing sequence: outdoors_slalom_00\n",
      "Loaded poses for person_1, shape: (333, 72)\n",
      "valid poses shape (328, 72)\n",
      "Processing sequence: outdoors_slalom_01\n",
      "Loaded poses for person_1, shape: (371, 72)\n",
      "valid poses shape (369, 72)\n"
     ]
    }
   ],
   "source": [
    "# Load a sequence file and its corresponding pose predictions\n",
    "def process_sequence(seq_name):\n",
    "    print(f\"Processing sequence: {seq_name}\")\n",
    "    \n",
    "    # Load sequence data\n",
    "    seq_file = os.path.join(dataset_dir, 'sequenceFiles', 'sequenceFiles', f'{DATASET_TYPE}', seq_name + '.pkl')\n",
    "    seq = pkl.load(open(seq_file, 'rb'), encoding='latin1')\n",
    "\n",
    "    # Iterate over person folders in the sequence\n",
    "    seq_pred_dir = os.path.join(base_pred_dir, seq_name, 'MotionBert')\n",
    "    motionbert_preds = os.path.join(seq_pred_dir, 'X3D.npy')\n",
    "    person_folders = [f for f in os.listdir(seq_pred_dir) if f.startswith(\"person_\")]\n",
    "\n",
    "    for person_folder in person_folders:\n",
    "        person_number = int(person_folder[-1])\n",
    "        person_path = os.path.join(seq_pred_dir, person_folder)\n",
    "        motionbert_preds = np.load(os.path.join(seq_pred_dir, person_folder, 'X3D.npy'))\n",
    "        predicted_translation, predicted_poses = calculate_smpl_output(motionbert_preds)\n",
    "\n",
    "        print(f\"Loaded poses for {person_folder}, shape: {predicted_poses.shape}\")\n",
    "\n",
    "        # Load the model \n",
    "        if person_number == 1:\n",
    "            iModel = 0  # Assuming one model for simplicity\n",
    "        elif person_number == 2:\n",
    "            iModel = 1\n",
    "        else: raise ValueError\n",
    "\n",
    "        if seq['genders'][iModel] == 'm':\n",
    "            model_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\readme\\smpl\\models\\basicmodel_m_lbs_10_207_0_v1.0.0.pkl\"\n",
    "        else:\n",
    "            model_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\readme\\smpl\\models\\basicModel_f_lbs_10_207_0_v1.0.0.pkl\"\n",
    "        \n",
    "        model = load_model(model_path)\n",
    "        model.betas[:10] = np.zeros(10)  # Use zero betas or customize if needed\n",
    "\n",
    "        valid_poses = []\n",
    "        valid_transitions = []\n",
    "\n",
    "        # Calculate vertices for all frames\n",
    "        vertices_all = []\n",
    "        for iFrame in range(len(seq['campose_valid'][iModel])):\n",
    "            if seq['campose_valid'][iModel][iFrame]:\n",
    "                valid_poses.append(predicted_poses[iFrame])\n",
    "                valid_transitions.append(predicted_translation[iFrame])\n",
    "\n",
    "                model.pose[:] = predicted_poses[iFrame]\n",
    "                model.trans[:] = predicted_translation[iFrame]\n",
    "                vertices = np.asarray(model)\n",
    "                vertices_all.append(vertices)\n",
    "        \n",
    "        # calculate vertices\n",
    "\n",
    "        print('valid poses shape', np.asarray(valid_poses).shape)\n",
    "\n",
    "        # Save the vertices for the person\n",
    "        vertices_all = np.array(vertices_all)  # Convert to numpy array\n",
    "        # output_file = os.path.join(person_path, 'vertices.npy')\n",
    "        # np.save(output_file, vertices_all)\n",
    "        # print(f\"Saved vertices to {output_file}, shape: {vertices_all.shape}\")\n",
    "\n",
    "        # all_thetas = predicted_poses[seq['campose_valid'][iModel]]\n",
    "        # all_translation = predicted_translation[seq['campose_valid'][iModel]]\n",
    "\n",
    "        # print('translation shape is', np.asarray(all_translation).shape)\n",
    "        # Define the rotation matrix\n",
    "        R_x = np.array([\n",
    "            [1,  0,  0],\n",
    "            [0, -1,  0],\n",
    "            [0,  0, -1]\n",
    "        ])\n",
    "\n",
    "        # Assuming verts_all is already defined with shape (1262, 6890, 3)\n",
    "        # Apply the rotation\n",
    "        #vertices_all = np.einsum('ij,klj->kli', R_x, vertices_all)\n",
    "\n",
    "        save_smpl(translations=predicted_translation, thetas=predicted_poses, vertices=np.asarray(vertices_all), output_folder = person_path)\n",
    "\n",
    "# Main processing function\n",
    "def process_all_sequences():\n",
    "    # List all sequence folders\n",
    "    sequence_folders = [f for f in os.listdir(base_pred_dir) if os.path.isdir(os.path.join(base_pred_dir, f))]\n",
    "    \n",
    "    for seq_name in sequence_folders:\n",
    "        process_sequence(seq_name)\n",
    "\n",
    "# Run the processing\n",
    "process_all_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT SUCH THAT THETAS AND TRANSLATION ARE ALSO CALCULATED IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_folders = [f for f in os.listdir(base_pred_dir) if os.path.isdir(os.path.join(base_pred_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes the name of the sequence and the person number and returns the boolean array of compose valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECOND WAY OF EXTRACTING VERTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_campose_valid(seq_name, person_nr):\n",
    "    seq_file = os.path.join(dataset_dir, 'sequenceFiles', 'sequenceFiles', 'test', seq_name + '.pkl')\n",
    "    seq = pkl.load(open(seq_file, 'rb'), encoding='latin1')\n",
    "    \n",
    "    \n",
    "    # # Iterate over person folders in the sequence\n",
    "    # seq_pred_dir = os.path.join(base_pred_dir, seq_name, 'MotionBert')\n",
    "    # person_folders = [f for f in os.listdir(seq_pred_dir) if f.startswith(\"person_\")]\n",
    "\n",
    "    # for person_folder in person_folders:\n",
    "    #     person_number = int(person_folder[-1])\n",
    "    #     person_path = os.path.join(seq_pred_dir, person_folder)\n",
    "\n",
    "    # Load the model (assuming only one model is needed per person)\n",
    "    if person_nr == 1:\n",
    "        iModel = 0  # Assuming one model for simplicity\n",
    "    elif person_nr == 2:\n",
    "        iModel = 1\n",
    "    else: raise ValueError\n",
    "\n",
    "    return seq['campose_valid'][iModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all motionbert files and calculate all SMPL parameters (+ vertices)\n",
    "\n",
    "def process_motionbert_files(root_path):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        for file in filenames:\n",
    "            if file == 'X3D.npy':\n",
    "                # Full path to the data file\n",
    "                data_path = os.path.join(dirpath, file)\n",
    "\n",
    "                # Call calculate_smpl_output\n",
    "                data = np.load(data_path)\n",
    "                translations, thetas = calculate_smpl_output(data)\n",
    "\n",
    "\n",
    "                thetas = torch.tensor(thetas).to('cuda')\n",
    "\n",
    "                all_translations_tensor = torch.tensor(translations, dtype=torch.double).to('cuda')\n",
    "\n",
    "                path_split_list = dirpath.split(os.sep)\n",
    "                seq_name =  path_split_list[-3]\n",
    "                person_nr = int(path_split_list[-1][-1]) \n",
    "                seq_file = os.path.join(dataset_dir, 'sequenceFiles', 'sequenceFiles', 'test', seq_name + '.pkl')\n",
    "                seq = pkl.load(open(seq_file, 'rb'), encoding='latin1')\n",
    "\n",
    "                # campose_valid = get_campose_valid(seq_name, person_nr)\n",
    "                # #print(len(campose_valid))\n",
    "\n",
    "                # all_translations_tensor = all_translations_tensor[campose_valid]\n",
    "                # thetas = thetas[campose_valid]\n",
    "\n",
    "\n",
    "                seq_file = os.path.join(dataset_dir, 'sequenceFiles', 'sequenceFiles', 'test', seq_name + '.pkl')\n",
    "                seq = pkl.load(open(seq_file, 'rb'), encoding='latin1')\n",
    "\n",
    "\n",
    "                output_flip_smpl = smpl(\n",
    "                    body_pose=thetas[:, 3:],\n",
    "                    global_orient=thetas[:, :3],\n",
    "                    pose2rot=True,\n",
    "                    create_transl=True,\n",
    "                    transl = all_translations_tensor\n",
    "                )\n",
    "\n",
    "                verts_all = output_flip_smpl.vertices.detach()\n",
    "\n",
    "                verts_all = verts_all.cpu().numpy()\n",
    "                output_folder = dirpath  # Save in the same directory\n",
    "\n",
    "\n",
    "                #render_and_save(verts_all, osp.join(output_folder, 'mesh.mp4'), keep_imgs=False, fps=fps_in, draw_face=True)\n",
    "\n",
    "                translations = all_translations_tensor.cpu().numpy()\n",
    "\n",
    "                thetas = thetas.cpu().numpy()\n",
    "\n",
    "                # # Define the rotation matrix\n",
    "                # R_x = np.array([\n",
    "                #     [1,  0,  0],\n",
    "                #     [0, -1,  0],\n",
    "                #     [0,  0, -1]\n",
    "                # ])\n",
    "\n",
    "                # # Assuming verts_all is already defined with shape (1262, 6890, 3)\n",
    "                # # Apply the rotation\n",
    "                # verts_all = np.einsum('ij,klj->kli', R_x, verts_all)\n",
    "\n",
    "                #Save outputs in the same directory as the data file\n",
    "                \n",
    "                save_smpl(translations, thetas, verts_all, output_folder)\n",
    "\n",
    "# Root folder containing all test sets\n",
    "root_folder = base_pred_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all motionbert files and calculate all SMPL parameters (+ vertices)\n",
    "\n",
    "def process_motionbert_files(root_path):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        for file in filenames:\n",
    "            if file == 'X3D.npy':\n",
    "                # Full path to the data file\n",
    "                data_path = os.path.join(dirpath, file)\n",
    "\n",
    "                # Call calculate_smpl_output\n",
    "                data = np.load(data_path)\n",
    "                translations, thetas = calculate_smpl_output(data)\n",
    "\n",
    "\n",
    "                thetas = torch.tensor(thetas).to('cuda')\n",
    "\n",
    "                all_translations_tensor = torch.tensor(translations, dtype=torch.double).to('cuda')\n",
    "\n",
    "                path_split_list = dirpath.split(os.sep)\n",
    "                seq_name =  path_split_list[-3]\n",
    "                #print('SEQ name is:', seq_name)\n",
    "                person_nr = int(path_split_list[-1][-1]) \n",
    "\n",
    "                # campose_valid = get_campose_valid(seq_name, person_nr)\n",
    "                # #print(len(campose_valid))\n",
    "\n",
    "                # all_translations_tensor = all_translations_tensor[campose_valid]\n",
    "                # thetas = thetas[campose_valid]\n",
    "\n",
    "\n",
    "                output_flip_smpl = smpl(\n",
    "                    body_pose=thetas[:, 3:],\n",
    "                    global_orient=thetas[:, :3],\n",
    "                    pose2rot=True,\n",
    "                    create_transl=True,\n",
    "                    transl = all_translations_tensor\n",
    "                )\n",
    "\n",
    "                verts_all = output_flip_smpl.vertices.detach()\n",
    "\n",
    "                verts_all = verts_all.cpu().numpy()\n",
    "                output_folder = dirpath  # Save in the same directory\n",
    "\n",
    "\n",
    "                #render_and_save(verts_all, osp.join(output_folder, 'mesh.mp4'), keep_imgs=False, fps=fps_in, draw_face=True)\n",
    "\n",
    "                translations = all_translations_tensor.cpu().numpy()\n",
    "\n",
    "                thetas = thetas.cpu().numpy()\n",
    "\n",
    "                # Define the rotation matrix\n",
    "                R_x = np.array([\n",
    "                    [1,  0,  0],\n",
    "                    [0, -1,  0],\n",
    "                    [0,  0, -1]\n",
    "                ])\n",
    "\n",
    "                # Assuming verts_all is already defined with shape (1262, 6890, 3)\n",
    "                # Apply the rotation\n",
    "                verts_all = np.einsum('ij,klj->kli', R_x, verts_all)\n",
    "\n",
    "                #Save outputs in the same directory as the data file\n",
    "                \n",
    "                save_smpl(translations, thetas, verts_all, output_folder)\n",
    "\n",
    "# Root folder containing all test sets\n",
    "root_folder = base_pred_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_motionbert_files(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.load(r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\all_thetas.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895, 72)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENDER AND LOOK WHAT TF IS WRONG Nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\vertices.npy\"\n",
    "vertices = np.load(file_path)\n",
    "file_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\vertices_all_frames.npy\"\n",
    "vertices_all_frames = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.50774313, 0.47102964, 0.21845487],\n",
       "        [0.49936341, 0.45677437, 0.2236435 ],\n",
       "        [0.51150523, 0.45228129, 0.21643265],\n",
       "        ...,\n",
       "        [0.40190039, 0.44389135, 0.09318051],\n",
       "        [0.40134032, 0.44518407, 0.09518934],\n",
       "        [0.3981391 , 0.44257733, 0.09703511]],\n",
       "\n",
       "       [[0.50848683, 0.47181244, 0.21798028],\n",
       "        [0.50018968, 0.45753138, 0.22317786],\n",
       "        [0.5123067 , 0.45309245, 0.21590144],\n",
       "        ...,\n",
       "        [0.40211151, 0.44448478, 0.09316706],\n",
       "        [0.40155601, 0.44577074, 0.09518098],\n",
       "        [0.39837849, 0.44315236, 0.09703512]],\n",
       "\n",
       "       [[0.51024218, 0.47144827, 0.21662677],\n",
       "        [0.50202228, 0.45714374, 0.2218423 ],\n",
       "        [0.5140904 , 0.45274851, 0.2144682 ],\n",
       "        ...,\n",
       "        [0.40295759, 0.44413438, 0.09252538],\n",
       "        [0.40241282, 0.44541567, 0.09454517],\n",
       "        [0.39925829, 0.4427856 , 0.09641637]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.26953391, 0.49526304, 0.28865103],\n",
       "        [0.26386925, 0.48089427, 0.29595449],\n",
       "        [0.27578705, 0.47760246, 0.28798839],\n",
       "        ...,\n",
       "        [0.15948517, 0.43926109, 0.1780694 ],\n",
       "        [0.15895979, 0.44060324, 0.18005142],\n",
       "        [0.15638861, 0.43784372, 0.18233366]],\n",
       "\n",
       "       [[0.26816745, 0.49491431, 0.28913624],\n",
       "        [0.26241881, 0.48052951, 0.29636947],\n",
       "        [0.27440264, 0.47723255, 0.28849851],\n",
       "        ...,\n",
       "        [0.15906637, 0.43926459, 0.17750496],\n",
       "        [0.15852609, 0.44060747, 0.17948096],\n",
       "        [0.15592717, 0.43784966, 0.18173786]],\n",
       "\n",
       "       [[0.26705733, 0.49456593, 0.2893202 ],\n",
       "        [0.26124522, 0.48016868, 0.29649579],\n",
       "        [0.2732749 , 0.47686648, 0.2886927 ],\n",
       "        ...,\n",
       "        [0.1586394 , 0.43925366, 0.17690749],\n",
       "        [0.15808896, 0.44059754, 0.17887867],\n",
       "        [0.15546921, 0.43784175, 0.1811158 ]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895, 6890, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_1\\vertices_all_frames.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_all_frames = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895, 6890, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices_all_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:57<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "render_and_save(vertices_all_frames[0:200], osp.join(r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\flat_guitar_01\\test', 'mesh.mp4'), keep_imgs=False, fps=fps_in, draw_face=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:42<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "render_and_save(vertices[0:200], osp.join(r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\flat_guitar_01\\test', 'mesh2.mp4'), keep_imgs=False, fps=fps_in, draw_face=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: USE OLD WAY OF EXTRACTING THE VERTICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\vertices.npy\"\n",
    "vertices = np.load(file_path)\n",
    "file_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_arguing_00\\MotionBert\\person_2\\vertices_all_frames.npy\"\n",
    "vertices_all_frames = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895, 6890, 3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE TO ME: TRY TWO THINGS: FILTER VERTICES EXTRACTED HERE BY COMPOSE VALID (look at linux extracting) OR: USE OLD, OTHER WAY OF EXTRACTING THE VERTICES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_pw3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
