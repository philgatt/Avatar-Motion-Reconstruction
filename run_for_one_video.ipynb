{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALPHAPOSE IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import natsort\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "alphapose_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\"\n",
    "\n",
    "if alphapose_path  not in sys.path:\n",
    "    sys.path.append(alphapose_path)\n",
    "\n",
    "from detector.apis import get_detector\n",
    "from trackers.tracker_api import Tracker\n",
    "from trackers.tracker_cfg import cfg as tcfg\n",
    "from trackers import track\n",
    "from alphapose.models import builder\n",
    "from alphapose.utils.config import update_config\n",
    "from alphapose.utils.detector import DetectionLoader\n",
    "from alphapose.utils.file_detector import FileDetectionLoader\n",
    "from alphapose.utils.transforms import flip, flip_heatmap\n",
    "from alphapose.utils.vis import getTime\n",
    "from alphapose.utils.webcam_detector import WebCamDetectionLoader\n",
    "from alphapose.utils.writer import DataWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOTIONBERT IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "motionbert_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBERT\\MotionBERT\"\n",
    "\n",
    "if motionbert_path  not in sys.path:\n",
    "    sys.path.append(motionbert_path)\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.utils.tools import *\n",
    "from lib.utils.learning import *\n",
    "from lib.utils.utils_data import flip_data\n",
    "from lib.data.dataset_wild import WildDetDataset\n",
    "from lib.utils.vismo import render_and_save\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import errno\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from lib.utils.tools import *\n",
    "from lib.model.loss import *\n",
    "from lib.model.loss_mesh import *\n",
    "from lib.utils.utils_mesh import *\n",
    "from lib.utils.utils_smpl import *\n",
    "from lib.utils.utils_data import *\n",
    "from lib.utils.learning import *\n",
    "from lib.data.dataset_mesh import MotionSMPL\n",
    "from lib.model.model_mesh import MeshRegressor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lib.data.augmentation import Augmenter3D\n",
    "from lib.utils.tools import read_pkl\n",
    "from lib.utils.utils_data import flip_data, crop_scale\n",
    "from lib.utils.utils_mesh import flip_thetas\n",
    "from lib.utils.utils_smpl import SMPL\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lib.data.datareader_h36m import DataReaderH36M  \n",
    "from lib.data.datareader_mesh import DataReaderMesh  \n",
    "from lib.data.dataset_action import random_move  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT ALPHAPOSE COORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Demo options\n",
    "        self.config = 'config.yaml'  # experiment configure file name\n",
    "        self.checkpoint = 'model.pth'  # checkpoint file name\n",
    "        self.sp = False  # Use single process for pytorch\n",
    "        self.detector = \"yolo\"  # detector name\n",
    "        self.detfile = \"\"  # detection result file\n",
    "        self.inputpath = \"input_images\"  # image-directory\n",
    "        self.inputlist = \"\"  # image-list\n",
    "        self.inputimg = \"example.jpg\"  # image-name\n",
    "        self.outputpath = \"examples/res/\"  # output-directory\n",
    "        self.save_img = False  # save result as image\n",
    "        self.vis = False  # visualize image\n",
    "        self.showbox = False  # visualize human bbox\n",
    "        self.profile = False  # add speed profiling at screen output\n",
    "        self.format = None  # save in format: coco/cmu/open\n",
    "        self.min_box_area = 0  # min box area to filter out\n",
    "        self.detbatch = 5  # detection batch size PER GPU\n",
    "        self.posebatch = 64  # pose estimation maximum batch size PER GPU\n",
    "        self.eval = False  # save result json as coco format with image index\n",
    "        self.gpus = \"0\"  # CUDA devices to use, or -1 for CPU\n",
    "        self.qsize = 1024  # length of result buffer\n",
    "        self.flip = False  # enable flip testing\n",
    "        self.debug = False  # print detail information\n",
    "\n",
    "        # Video options\n",
    "        self.video = \"\"  # video-name\n",
    "        self.webcam = -1  # webcam number\n",
    "        self.save_video = False  # whether to save rendered video\n",
    "        self.vis_fast = False  # use fast rendering\n",
    "\n",
    "        # Tracking options\n",
    "        self.pose_flow = False  # track humans in video with PoseFlow\n",
    "        self.pose_track = False  # track humans in video with reid\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.cfg = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\configs\\halpe_26\\resnet\\256x192_res50_lr1e-3_1x_jupyter.yaml'\n",
    "args.checkpoint  = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\pretrained_models\\halpe26_fast_res50_256x192.pth\"\n",
    "args.video = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\downtown_bus_00\\downtown_bus_00.mp4\"\n",
    "args.outputpath = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\\downtown_bus_00'\n",
    "args.save_video = False\n",
    "args.vis_fast = False\n",
    "\n",
    "cfg = update_config(args.cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_finish_info():\n",
    "    print('===========================> Finish Model Running.')\n",
    "    if (args.save_img or args.save_video) and not args.vis_fast:\n",
    "        print('===========================> Rendering remaining images in the queue...')\n",
    "        print('===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions will be done on device:\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    args.sp = True\n",
    "\n",
    "args.gpus = [int(i) for i in args.gpus.split(',')] if torch.cuda.device_count() >= 1 else [-1]\n",
    "args.device = torch.device(\"cuda:\" + str(args.gpus[0]) if args.gpus[0] >= 0 else \"cpu\")\n",
    "args.detbatch = args.detbatch * len(args.gpus)\n",
    "args.posebatch = args.posebatch * len(args.gpus)\n",
    "args.tracking = args.pose_track or args.pose_flow or args.detector=='tracker'\n",
    "\n",
    "print(\"Predictions will be done on device:\")\n",
    "print(args.device)\n",
    "\n",
    "if not args.sp:\n",
    "    torch.multiprocessing.set_start_method('forkserver', force=True)\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test folders: ['downtown_arguing_00', 'downtown_bar_00', 'downtown_bus_00', 'downtown_cafe_00', 'downtown_car_00', 'downtown_crossStreets_00', 'downtown_downstairs_00', 'downtown_enterShop_00', 'downtown_rampAndStairs_00', 'downtown_runForBus_00', 'downtown_runForBus_01', 'downtown_sitOnStairs_00', 'downtown_stairs_00', 'downtown_upstairs_00', 'downtown_walkBridge_01', 'downtown_walking_00', 'downtown_walkUphill_00', 'downtown_warmWelcome_00', 'downtown_weeklyMarket_00', 'downtown_windowShopping_00', 'flat_guitar_01', 'flat_packBags_00', 'office_phoneCall_00', 'outdoors_fencing_01']\n",
      "Train folders: ['courtyard_arguing_00', 'courtyard_backpack_00', 'courtyard_basketball_00', 'courtyard_bodyScannerMotions_00', 'courtyard_box_00', 'courtyard_capoeira_00', 'courtyard_captureSelfies_00', 'courtyard_dancing_01', 'courtyard_giveDirections_00', 'courtyard_golf_00', 'courtyard_goodNews_00', 'courtyard_jacket_00', 'courtyard_laceShoe_00', 'courtyard_rangeOfMotions_00', 'courtyard_relaxOnBench_00', 'courtyard_relaxOnBench_01', 'courtyard_shakeHands_00', 'courtyard_warmWelcome_00', 'outdoors_climbing_00', 'outdoors_climbing_01', 'outdoors_climbing_02', 'outdoors_freestyle_00', 'outdoors_slalom_00', 'outdoors_slalom_01']\n",
      "Validation folders: ['courtyard_basketball_01', 'courtyard_dancing_00', 'courtyard_drinking_00', 'courtyard_hug_00', 'courtyard_jumpBench_01', 'courtyard_rangeOfMotions_01', 'downtown_walkDownhill_00', 'outdoors_crosscountry_00', 'outdoors_freestyle_01', 'outdoors_golf_00', 'outdoors_parcours_00', 'outdoors_parcours_01']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Initialize lists for test, train, and validation folders\n",
    "test_folders = []\n",
    "train_folders = []\n",
    "val_folders = []\n",
    "\n",
    "# Base directory path\n",
    "base_dir = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\sequenceFiles\\sequenceFiles\"\n",
    "\n",
    "# Subfolder names\n",
    "subfolders = {\n",
    "    \"test\": test_folders,\n",
    "    \"train\": train_folders,\n",
    "    \"validation\": val_folders\n",
    "}\n",
    "\n",
    "# Iterate over each subfolder\n",
    "for folder_name, file_list in subfolders.items():\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if os.path.exists(folder_path):  # Check if the folder exists\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".pkl\"):\n",
    "                file_list.append(os.path.splitext(file)[0])  # Add the filename without extension\n",
    "\n",
    "# Print the lists\n",
    "print(\"Test folders:\", test_folders)\n",
    "print(\"Train folders:\", train_folders)\n",
    "print(\"Validation folders:\", val_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\"\n",
    "output_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "\n",
    "# Step 1: Traverse directories and find all the .mp4 files\n",
    "video_paths = glob.glob(os.path.join(input_base_path, \"**\", \"*.mp4\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\train_set\"\n",
    "output_test_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\"\n",
    "output_val_base_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\val_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ENTER VIDEO PATH AND OUTPUT PATH\n",
    "\n",
    "video_paths = [r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\downtown_arguing_00\\downtown_arguing_00.mp4\"]\n",
    "output_dir = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\main\\output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x000001DBECADAC48>\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO detector...\n",
      "{'CONFIG': 'E:/Users/Philipp/Dokumente/Pose_Estimation_3D/Alphapose/AlphaPose/detector/yolo/cfg/yolov3-spp.cfg', 'WEIGHTS': 'E:/Users/Philipp/Dokumente/Pose_Estimation_3D/Alphapose/AlphaPose/detector/yolo/data/yolov3-spp.weights', 'INP_DIM': 608, 'NMS_THRES': 0.6, 'CONFIDENCE': 0.1, 'NUM_CLASSES': 80}\n",
      "Loading pose model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda-envs\\main_pw3d\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "e:\\miniconda-envs\\main_pw3d\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n",
      "Processing video: E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\3DPW\\imageFiles\\imageFiles\\downtown_arguing_00\\downtown_arguing_00.mp4\n",
      "Saving in E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Alphapose\\AlphaPose\\examples\\res\\test_set\n",
      "yolo IS <detector.yolo_api.YOLODetector object at 0x000001DBECA7ADC8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/898 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n",
      "E:/Users/Philipp/Dokumente/Pose_Estimation_3D/Alphapose/AlphaPose/detector/yolo/cfg/yolov3-spp.cfg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 622/898 [00:39<00:06, 42.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898/898 [00:45<00:00, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Finish Model Running.\n",
      "Results have been written to json.ring remaining 0 images in the queue...\n",
      "WARNING; STILL USING ABS PATH FOR DETECTOR\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def initialize_models():\n",
    "    \"\"\"Load YOLO and Pose models once.\"\"\"\n",
    "    print(\"Loading YOLO detector...\")\n",
    "    yolo_detector = get_detector(args)  # Replace with actual detector initialization\n",
    "\n",
    "    print(\"Loading pose model...\")\n",
    "    pose_model = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n",
    "    pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n",
    "    if len(args.gpus) > 1:\n",
    "        pose_model = torch.nn.DataParallel(pose_model, device_ids=args.gpus).to(args.device)\n",
    "    else:\n",
    "        pose_model.to(args.device)\n",
    "    pose_model.eval()\n",
    "    print(\"Models loaded successfully.\")\n",
    "    return yolo_detector, pose_model\n",
    "\n",
    "i_ = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load models once\n",
    "    yolo_detector, pose_model = initialize_models()\n",
    "\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        \n",
    "        # # Generate the output directory structure\n",
    "        # relative_path = os.path.relpath(video_path, input_base_path)\n",
    "        # cur_folder = os.path.basename(os.path.dirname(video_path))\n",
    "        # if cur_folder in train_folders:\n",
    "        #     output_base_path = output_train_base_path\n",
    "        # elif cur_folder in test_folders:\n",
    "        #     output_base_path = output_test_base_path\n",
    "        # elif cur_folder in output_val_base_path:\n",
    "        #     output_base_path = output_val_base_path\n",
    "        # else:\n",
    "        #     print(f'Skipping folder {cur_folder}, because its set is not defined.')\n",
    "        #     continue\n",
    "\n",
    "        output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Set the output path for the specific video\n",
    "        args.outputpath = output_dir\n",
    "        input_source = video_path\n",
    "        \n",
    "        mode = \"video\"  # Assume videos for this example; adapt based on your needs\n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        print(f'Saving in {output_base_path}')\n",
    "\n",
    "        \n",
    "        # Load detection loader for the specific video\n",
    "        print('yolo IS', yolo_detector)\n",
    "        det_loader = DetectionLoader(input_source, yolo_detector, cfg, args, batchSize=args.detbatch, mode=mode, queueSize=args.qsize)\n",
    "        det_worker = det_loader.start()\n",
    "        \n",
    "        runtime_profile = {\n",
    "            'dt': [],\n",
    "            'pt': [],\n",
    "            'pn': []\n",
    "        }\n",
    "        \n",
    "        # Init data writer\n",
    "        queueSize = 2 if mode == 'webcam' else args.qsize\n",
    "        if args.save_video and mode != 'image':\n",
    "            from alphapose.utils.writer import DEFAULT_VIDEO_SAVE_OPT as video_save_opt\n",
    "            video_save_opt['savepath'] = os.path.join(args.outputpath, 'AlphaPose_' + os.path.basename(input_source))\n",
    "            video_save_opt.update(det_loader.videoinfo)\n",
    "            writer = DataWriter(cfg, args, save_video=True, video_save_opt=video_save_opt, queueSize=queueSize).start()\n",
    "        else:\n",
    "            writer = DataWriter(cfg, args, save_video=False, queueSize=queueSize).start()\n",
    "        \n",
    "        data_len = det_loader.length\n",
    "        im_names_desc = tqdm(range(data_len), dynamic_ncols=True)\n",
    "\n",
    "        batchSize = args.posebatch\n",
    "        if args.flip:\n",
    "            batchSize = int(batchSize / 2)\n",
    "        try:\n",
    "            for i in im_names_desc:\n",
    "                start_time = getTime()\n",
    "                with torch.no_grad():\n",
    "                    (inps, orig_img, im_name, boxes, scores, ids, cropped_boxes) = det_loader.read()\n",
    "                    # # Create a zero-filled tensor for keypoints\n",
    "                    # # Example of printing values for debugging\n",
    "                    # print(\"Inputs:\", inps.shape)                # Print inputs\n",
    "                    # print(\"Original Image:\", orig_img.shape)    # Print the original image\n",
    "                    # print(\"Image Name:\", im_name)          # Print the image name\n",
    "                    # print(\"Bounding Boxes:\", boxes.shape)       # Print bounding boxes\n",
    "                    # print(\"Scores:\", scores.shape)               # Print scores\n",
    "                    # print(\"IDs:\", ids.shape)                     # Print object IDs\n",
    "                    # print(\"Cropped Boxes:\", cropped_boxes.shape)  # Print cropped bounding boxes\n",
    "                    if orig_img is None:\n",
    "                        break\n",
    "                    if boxes is None or boxes.nelement() == 0:\n",
    "                        print('No boxes for frame', i)\n",
    "                        # Create a zero-filled tensor for keypoints\n",
    "                        # Example of printing values for debugging\n",
    "                        # print(\"Inputs:\", inps)                # Print inputs\n",
    "                        # print(\"Original Image:\", orig_img)    # Print the original image\n",
    "                        # print(\"Image Name:\", im_name)          # Print the image name\n",
    "                        # print(\"Bounding Boxes:\", boxes)       # Print bounding boxes\n",
    "                        # print(\"Scores:\", scores)               # Print scores\n",
    "                        # print(\"IDs:\", ids)                     # Print object IDs\n",
    "                        # print(\"Cropped Boxes:\", cropped_boxes)  # Print cropped bounding boxes\n",
    "                        # inps = torch.zeros((1, 3, 256, 192))\n",
    "                        # im_name = torch.zeros((1, 3, 256, 192))\n",
    "                        # boxes = torch.zeros((1, 4))  # Shape: [1, keypoints, (x, y, confidence)]\n",
    "                        # scores = torch.zeros((1, 1))\n",
    "                        # ids = torch.zeros((1, 1))\n",
    "                        # cropped_boxes = torch.zeros((1, 4))\n",
    "                        # writer.save(boxes, scores, ids, boxes, cropped_boxes, orig_img, im_name)\n",
    "                        prev_scores = torch.zeros_like(prev_scores)\n",
    "                        writer.save(prev_boxes, prev_scores, prev_ids, prev_hm, prev_cropped_boxes, orig_img, im_name)\n",
    "                        i_.append(i)\n",
    "                        continue\n",
    "                    if args.profile:\n",
    "                        ckpt_time, det_time = getTime(start_time)\n",
    "                        runtime_profile['dt'].append(det_time)\n",
    "                    # Pose Estimation\n",
    "                    inps = inps.to(args.device)\n",
    "                    datalen = inps.size(0)\n",
    "                    leftover = 0\n",
    "                    if (datalen) % batchSize:\n",
    "                        leftover = 1\n",
    "                    num_batches = datalen // batchSize + leftover\n",
    "                    hm = []\n",
    "                    if i == 1276:\n",
    "                        print(\"num batches\", num_batches)\n",
    "                    for j in range(num_batches):\n",
    "                        inps_j = inps[j * batchSize:min((j + 1) * batchSize, datalen)]\n",
    "                        if args.flip:\n",
    "                            inps_j = torch.cat((inps_j, flip(inps_j)))\n",
    "                        hm_j = pose_model(inps_j)\n",
    "                        if args.flip:\n",
    "                            hm_j_flip = flip_heatmap(hm_j[int(len(hm_j) / 2):], pose_dataset.joint_pairs, shift=True)\n",
    "                            hm_j = (hm_j[0:int(len(hm_j) / 2)] + hm_j_flip) / 2\n",
    "                        hm.append(hm_j)\n",
    "                        i_.append(i)\n",
    "                    hm = torch.cat(hm)\n",
    "                    if len(hm) == 0:\n",
    "                        print('length for', im_names_desc[i], 'is zero')\n",
    "                    if args.profile:\n",
    "                        ckpt_time, pose_time = getTime(ckpt_time)\n",
    "                        runtime_profile['pt'].append(pose_time)\n",
    "                    if args.pose_track:\n",
    "                        boxes, scores, ids, hm, cropped_boxes = track(tracker, args, orig_img, inps, boxes, hm, cropped_boxes, im_name, scores)\n",
    "                    # if i == 614:                  \n",
    "                    #     print(\"Boxes:\", boxes)\n",
    "                    #     print(\"Scores:\", scores)\n",
    "                    #     print(\"IDs:\", ids)\n",
    "                    #     print(\"Heatmap:\", hm)\n",
    "                    #     print(\"Cropped Boxes:\", cropped_boxes)\n",
    "                    #     print(\"Original Image:\", orig_img)\n",
    "                    #     print(\"Image Name:\", im_name)\n",
    "                        \n",
    "                    writer.save(boxes, scores, ids, hm, cropped_boxes, orig_img, im_name)\n",
    "\n",
    "                    prev_boxes = boxes\n",
    "                    prev_scores = scores\n",
    "                    prev_ids = ids\n",
    "                    prev_hm = hm\n",
    "                    prev_cropped_boxes = cropped_boxes\n",
    "                    if args.profile:\n",
    "                        ckpt_time, post_time = getTime(ckpt_time)\n",
    "                        runtime_profile['pn'].append(post_time)\n",
    "                \n",
    "                if args.profile:\n",
    "                    # TQDM\n",
    "                    im_names_desc.set_description(\n",
    "                        'det time: {dt:.4f} | pose time: {pt:.4f} | post processing: {pn:.4f}'.format(\n",
    "                            dt=np.mean(runtime_profile['dt']), pt=np.mean(runtime_profile['pt']), pn=np.mean(runtime_profile['pn']))\n",
    "                    )\n",
    "                    \n",
    "            print_finish_info()\n",
    "            while(writer.running()):\n",
    "                time.sleep(1)\n",
    "                print('===========================> Rendering remaining ' + str(writer.count()) + ' images in the queue...', end='\\r')\n",
    "            writer.stop()\n",
    "            det_loader.stop()\n",
    "        except Exception as e:\n",
    "            print(repr(e))\n",
    "            print('An error as above occurs when processing the images, please check it')\n",
    "            pass\n",
    "        except KeyboardInterrupt:\n",
    "            print_finish_info()\n",
    "            if args.sp:\n",
    "                det_loader.terminate()\n",
    "                while(writer.running()):\n",
    "                    time.sleep(1)\n",
    "                    print('===========================> Rendering remaining ' + str(writer.count()) + ' images in the queue...', end='\\r')\n",
    "                writer.stop()\n",
    "            else:\n",
    "                det_loader.terminate()\n",
    "                writer.terminate()\n",
    "                writer.clear_queues()\n",
    "                det_loader.clear_queues()\n",
    "    \n",
    "print('WARNING; STILL USING ABS PATH FOR DETECTOR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all variables\n",
    "#%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTER JSON RESULTS SO WE HAVE ONLY ONE DETECTION PER FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'output/alphapose-results.json' has been filtered and overwritten.\n"
     ]
    }
   ],
   "source": [
    "# Path to the JSON file\n",
    "file_path = 'output/alphapose-results.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Dictionary to store the first occurrence of each image_id\n",
    "unique_entries = {}\n",
    "\n",
    "# Iterate over the list of dictionaries\n",
    "for entry in data:\n",
    "    image_id = entry['image_id']\n",
    "    # Only add the entry if the image_id is not already in the dictionary\n",
    "    if image_id not in unique_entries:\n",
    "        unique_entries[image_id] = entry\n",
    "\n",
    "# Extract the unique entries as a list\n",
    "filtered_data = list(unique_entries.values())\n",
    "\n",
    "# Overwrite the file with the filtered data\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(filtered_data, file, indent=4)\n",
    "\n",
    "print(f\"File '{file_path}' has been filtered and overwritten.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOTIONBERT DETECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Define all your arguments as class attributes with default values\n",
    "        self.config = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBert\\MotionBERT\\configs\\pose3d\\MB_ft_h36m_global_lite.yaml\"  # Path to the config file\n",
    "        self.evaluate = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBert\\MotionBERT\\checkpoint\\pose3d\\FT_MB_lite_MB_ft_h36m_global_lite\\best_epoch.bin\"  # Checkpoint to evaluate\n",
    "        self.json_path = None  # Alphapose detection result JSON path\n",
    "        self.vid_path = None  # Video path\n",
    "        self.out_path = None  # Output path\n",
    "        self.pixel = False  # Align with pixel coordinates\n",
    "        self.focus = None  # Target person ID\n",
    "        self.clip_len = 243  # Clip length for network input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = Args()\n",
    "args = get_config(opts.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "model_backbone = load_backbone(args)\n",
    "if torch.cuda.is_available():\n",
    "    model_backbone = nn.DataParallel(model_backbone)\n",
    "    model_backbone = model_backbone.cuda()\n",
    "\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load(opts.evaluate, map_location=lambda storage, loc: storage)\n",
    "model_backbone.load_state_dict(checkpoint['model_pos'], strict=True)\n",
    "model_pos = model_backbone\n",
    "model_pos.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(opts):\n",
    "\n",
    "    testloader_params = {\n",
    "        'batch_size': 1,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 8,\n",
    "        'pin_memory': True,\n",
    "        'prefetch_factor': 4,\n",
    "        'persistent_workers': True,\n",
    "        'drop_last': False\n",
    "    }\n",
    "\n",
    "    vid = imageio.get_reader(opts.vid_path, 'ffmpeg')\n",
    "    fps_in = vid.get_meta_data()['fps']\n",
    "    vid_size = vid.get_meta_data()['size']\n",
    "    os.makedirs(opts.out_path, exist_ok=True)\n",
    "\n",
    "    if opts.pixel:\n",
    "        wild_dataset = WildDetDataset(opts.json_path, clip_len=opts.clip_len, vid_size=vid_size, scale_range=None, focus=opts.focus)\n",
    "    else:\n",
    "        wild_dataset = WildDetDataset(opts.json_path, clip_len=opts.clip_len, scale_range=[1, 1], focus=opts.focus)\n",
    "\n",
    "    test_loader = DataLoader(wild_dataset, **testloader_params)\n",
    "\n",
    "    results_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch_input in tqdm(test_loader):\n",
    "            N, T = batch_input.shape[:2]\n",
    "            if torch.cuda.is_available():\n",
    "                batch_input = batch_input.cuda()\n",
    "            if args.no_conf:\n",
    "                batch_input = batch_input[:, :, :, :2]\n",
    "            if args.flip:\n",
    "                batch_input_flip = flip_data(batch_input)\n",
    "                predicted_3d_pos_1 = model_pos(batch_input)\n",
    "                predicted_3d_pos_flip = model_pos(batch_input_flip)\n",
    "                predicted_3d_pos_2 = flip_data(predicted_3d_pos_flip)\n",
    "                predicted_3d_pos = (predicted_3d_pos_1 + predicted_3d_pos_2) / 2.0\n",
    "            else:\n",
    "                predicted_3d_pos = model_pos(batch_input)\n",
    "            if args.rootrel:\n",
    "                predicted_3d_pos[:, :, 0, :] = 0\n",
    "            else:\n",
    "                predicted_3d_pos[:, 0, 0, 2] = 0\n",
    "            if args.gt_2d:\n",
    "                predicted_3d_pos[..., :2] = batch_input[..., :2]\n",
    "            results_all.append(predicted_3d_pos.cpu().numpy())\n",
    "\n",
    "    results_all = np.hstack(results_all)\n",
    "    results_all = np.concatenate(results_all)\n",
    "    #render_and_save(results_all, f'{opts.out_path}/X3D.mp4', keep_imgs=False, fps=fps_in)\n",
    "    if opts.pixel:\n",
    "        results_all = results_all * (min(vid_size) / 2.0)\n",
    "        results_all[:, :, :2] = results_all[:, :, :2] + np.array(vid_size) / 2.0\n",
    "    np.save(f'{opts.out_path}/X3D.npy', results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: downtown_arguing_00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths\n",
    "video_path = video_paths[0]  # Replace with the actual video file path\n",
    "json_path = \"output/alphapose-results.json\"  # Path to the JSON file\n",
    "output_base_path = os.path.dirname(json_path)  # Save results in the same folder as the JSON file\n",
    "\n",
    "# Overwrite existing files\n",
    "overwrite = True  \n",
    "\n",
    "# Extract video and JSON information\n",
    "video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "output_path = os.path.join(output_base_path, \"MotionBert\", video_name)\n",
    "\n",
    "x3d_mp4 = os.path.join(output_path, \"X3D.mp4\")\n",
    "x3d_npy = os.path.join(output_path, \"X3D.npy\")\n",
    "\n",
    "# Process the video and JSON if necessary\n",
    "if overwrite or not (os.path.exists(x3d_mp4) and os.path.exists(x3d_npy)):\n",
    "    print(f\"Processing video: {video_name}\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Set up options for processing\n",
    "    opts.vid_path = video_path\n",
    "    opts.json_path = json_path\n",
    "    opts.out_path = output_path\n",
    "\n",
    "    # Call the main processing function\n",
    "    main(opts)\n",
    "\n",
    "    # Free up memory after processing\n",
    "else:\n",
    "    print(f\"Files already exist and overwrite is set to False: {x3d_mp4}, {x3d_npy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATING SMPL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_params_path = r\"E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\Pose_to_SMPL\\video2bvh\"\n",
    "if smpl_params_path not in sys.path:\n",
    "    sys.path .append(smpl_params_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pose_estimator_2d import openpose_estimator\n",
    "# from pose_estimator_3d import estimator_3d\n",
    "from smpl_utils import smooth, vis, camera\n",
    "from bvh_skeleton import openpose_skeleton, h36m_skeleton, cmu_skeleton\n",
    "\n",
    "import cv2\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "#import mathutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(output_path, \"X3D.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = [(0, None), (4, None), (1, None), (7, None), (5, None), (2, None), (3, None), (6, None), (12, None), (8, None), (15, None), (None, [0, 0, 0]), (9, None), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (10, None), (13, None), (11, None), (14, None), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0]), (None, [0, 0, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "h36m_skel = h36m_skeleton.H36mSkeleton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_to_quaternion_zxy(euler_z, euler_x, euler_y):\n",
    "    # Create a Rotation object with 'ZXY' order\n",
    "    r = R.from_euler('zxy', [euler_z, euler_x, euler_y], degrees=True)\n",
    "\n",
    "    # Convert to quaternion\n",
    "    quaternion = r.as_quat()  # Returns (x, y, z, w)\n",
    "\n",
    "    # Return as a tuple in the desired order (W, Y, Z, X)\n",
    "    return (quaternion[3], quaternion[1], quaternion[2], quaternion[0])\n",
    "\n",
    "\n",
    "def rodrigues_from_pose(frame, joint_name, all_rotations):\n",
    "    # Extract quaternion from input dictionary\n",
    "    quat = all_rotations[frame][joint_name]  # Assumes quaternion is in (W, X, Y, Z) order\n",
    "\n",
    "    # Create a SciPy Rotation object\n",
    "    rotation = R.from_quat([quat[1], quat[2], quat[3], quat[0]])  # Convert to (X, Y, Z, W)\n",
    "\n",
    "    # Convert to axis-angle representation\n",
    "    axis = rotation.as_rotvec()  # Returns Rodrigues vector directly (axis * angle)\n",
    "\n",
    "    return axis\n",
    "\n",
    "\n",
    "def apply_mapping(B, mapping, group_size=3):\n",
    "    \"\"\"\n",
    "    Apply a mapping to reorder and pad a list like B.\n",
    "    \"\"\"\n",
    "    # Divide B into groups of group_size\n",
    "    grouped_B = [B[i:i + group_size] for i in range(0, len(B), group_size)]\n",
    "\n",
    "    # Apply mapping\n",
    "    output_groups = []\n",
    "    for index, padding in mapping:\n",
    "        if index is not None:\n",
    "            output_groups.append(grouped_B[index])  # Reorder from B\n",
    "        elif padding is not None:\n",
    "            output_groups.append(padding)  # Add padding group\n",
    "\n",
    "    # Flatten the output groups\n",
    "    reordered_B = [item for group in output_groups for item in group]\n",
    "    return reordered_B\n",
    "\n",
    "\n",
    "def load_and_prepare_data(data):\n",
    "\n",
    "    # Load the 3D pose data from the .npy file\n",
    "    pose_data = data\n",
    "\n",
    "    # Get the number of frames and joints\n",
    "    n_frames, n_joints, _ = pose_data.shape\n",
    "\n",
    "    # Define the rotation matrix for -90 degrees around the x-axis\n",
    "    rotation_matrix = np.array([\n",
    "        [1,  0,  0],\n",
    "        [0,  0, 1],\n",
    "        [0,  -1,  0]\n",
    "    ])\n",
    "\n",
    "    # Apply the rotation matrix to every frame and every joint\n",
    "    # We reshape the pose_data to (n_frames * n_joints, 3), apply the rotation, then reshape back\n",
    "    data = np.dot(pose_data.reshape(-1, 3), rotation_matrix.T).reshape(n_frames, n_joints, 3)\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def calculate_translations(channels):\n",
    "\n",
    "    all_translations = []\n",
    "\n",
    "    for index in range(len(channels)):\n",
    "        # Initialize the dictionary to hold the results\n",
    "        # Iterate through the list and extract translation values\n",
    "        translation_values = channels[index][0:3]  # Extracting the translation values\n",
    "\n",
    "        all_translations.append(translation_values)\n",
    "\n",
    "    return all_translations\n",
    "\n",
    "\n",
    "def calculate_rotations(channels, body_parts):\n",
    "\n",
    "    all_rotations = []\n",
    "\n",
    "    for index in range(len(channels)):\n",
    "        # Initialize the dictionary to hold the results\n",
    "        rotation_dict = {}\n",
    "        # Iterate through the list and extract rotation values\n",
    "        rotation_values = channels[index][3:]  # Extracting the rotation values, skipping the first two\n",
    "        if index == 0:\n",
    "            print(rotation_values)\n",
    "        for i, body_part in enumerate(body_parts):\n",
    "            # Take 3 consecutive values (x, y, z rotations) for each body part\n",
    "            quat = euler_to_quaternion_zxy(-rotation_values[i*3+2], rotation_values[i*3+1], rotation_values[i*3])\n",
    "            rotation_dict[body_part] = quat\n",
    "\n",
    "        all_rotations.append(rotation_dict)\n",
    "\n",
    "    return all_rotations\n",
    "\n",
    "def calculate_all_poses(all_rotations, body_parts):\n",
    "\n",
    "    joint_names = body_parts\n",
    "    num_joints = len(joint_names)\n",
    "    frame = 0\n",
    "    all_poses = []\n",
    "\n",
    "\n",
    "    for frame in range(len(all_rotations)):\n",
    "        # Get armature pose in rodrigues representation\n",
    "        pose = [0.0] * (num_joints * 3)\n",
    "\n",
    "        for index in range(num_joints):\n",
    "            joint_name = joint_names[index]\n",
    "            joint_pose = rodrigues_from_pose(frame, joint_name, all_rotations)\n",
    "            pose[index*3 + 0] = joint_pose[2]\n",
    "            pose[index*3 + 1] = joint_pose[0]\n",
    "            pose[index*3 + 2] = joint_pose[1]\n",
    "        \n",
    "        all_poses.append(pose)\n",
    "\n",
    "    return all_poses\n",
    "\n",
    "def calculate_all_thetas(all_poses):\n",
    "    all_thetas = []\n",
    "\n",
    "    for i, frame in enumerate(all_poses):\n",
    "        theta_calculated = all_poses[i]\n",
    "\n",
    "        reordered_theta = apply_mapping(theta_calculated, mapping)\n",
    "        smpl_h_params = reordered_theta\n",
    "\n",
    "        reordered_theta = reordered_theta[0:72]\n",
    "\n",
    "        all_thetas.append(reordered_theta)\n",
    "    return all_thetas\n",
    "\n",
    "    #np.save(\"all_thetas_backpack.npy\", all_thetas)\n",
    "\n",
    "def save_smpl(translations, thetas, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    np.save(os.path.join(output_folder, 'all_translations.npy'), translations)\n",
    "    np.save(os.path.join(output_folder, 'all_thetas.npy'), thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smpl_output(data):\n",
    "\n",
    "    data = load_and_prepare_data(data)\n",
    "\n",
    "    channels, header = h36m_skel.poses2bvh(data, output_file=None)\n",
    "\n",
    "    all_translations = calculate_translations(channels)\n",
    "\n",
    "    # List of body parts in the order matching the data\n",
    "    body_parts = [\n",
    "        'Hip', 'RightHip', 'RightKnee', 'RightAnkle', 'LeftHip', 'LeftKnee', 'LeftAnkle', \n",
    "        'Spine', 'Thorax', 'Neck', 'LeftShoulder', 'LeftElbow', 'LeftWrist', \n",
    "        'RightShoulder', 'RightElbow', 'RightWrist'\n",
    "    ]\n",
    "\n",
    "    all_rotations = calculate_rotations(channels, body_parts)\n",
    "\n",
    "    all_poses = calculate_all_poses(all_rotations, body_parts)\n",
    "\n",
    "\n",
    "    all_thetas = calculate_all_thetas(all_poses)\n",
    "\n",
    "    return all_translations, all_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.955118798636084, 8.701388079745376, 0.0839545243032732, -6.3261396073775895, 4.813230566176596, -2.060382271891259, 0.5383237074696723, 9.274848405909221, -0.05518970066172768, 0.0, 0.0, -3.975693351829396e-16, 5.621375902514069, 8.322897743671055, 6.449372094615479, 0.24973031895028158, 7.310425286246511, -0.5426711837412692, 0.0, 0.0, -9.93923337957349e-17, 0.014359744086693058, 3.2098695109470325, 0.6649276816192868, 5.436541540770314, 7.08804992527409e-16, -2.3266151989600244e-16, 3.942120226588329, 6.6266646865177785, 1.4146543000369616, -9.290704116815983, 12.745695001072127, 85.01356417959325, -32.513705065117946, -2.3541352146956868e-15, -8.96902648485639e-15, 4.138037154422024e-33, -1.5902773407317584e-15, 2.981770013872047e-16, 22.83370801185375, 12.432785558275128, -85.20239581849702, 18.766208534534712, -5.679265551022281e-15, -5.105936804101817e-15, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "all_translations, all_thetas = calculate_smpl_output(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMPL PARAMS TO VERTICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(898, 72)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(all_thetas).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_root = r'E:\\Users\\Philipp\\Dokumente\\Pose_Estimation_3D\\MotionBert\\MotionBERT\\data\\mesh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "smpl = SMPL(args.data_root, batch_size=1).double().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_thetas = torch.tensor(all_thetas).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_translations_tensor = torch.tensor(all_translations, dtype=torch.double).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_flip_smpl = smpl(\n",
    "    body_pose=all_thetas[:, 3:],\n",
    "    global_orient=all_thetas[:, :3],\n",
    "    pose2rot=True,\n",
    "    create_transl=True,\n",
    "    transl = all_translations_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = imageio.get_reader(opts.vid_path,  'ffmpeg')\n",
    "fps_in = vid.get_meta_data()['fps']\n",
    "vid_size = vid.get_meta_data()['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_all = output_flip_smpl.vertices.detach()\n",
    "\n",
    "verts_all = verts_all.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.vid_path = video_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output\\\\MotionBert\\\\downtown_arguing_00'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts.out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(898, 6890, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the rotation matrix\n",
    "# R_x = np.array([\n",
    "#     [1,  0,  0],\n",
    "#     [0, -1,  0],\n",
    "#     [0,  0, -1]\n",
    "# ])\n",
    "\n",
    "# # Assuming verts_all is already defined with shape (1262, 6890, 3)\n",
    "# # Apply the rotation\n",
    "# verts_all = np.einsum('ij,klj->kli', R_x, verts_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_all = verts_all[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [04:19<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "render_and_save(verts_all, osp.join(opts.out_path, 'mesh.mp4'), keep_imgs=False, fps=fps_in, draw_face=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output\\\\MotionBert\\\\downtown_arguing_00'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts.out_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_pw3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
